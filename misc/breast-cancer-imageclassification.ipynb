{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1873742,"sourceType":"datasetVersion","datasetId":1115384}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-13T06:24:50.593953Z","iopub.execute_input":"2025-02-13T06:24:50.594199Z","iopub.status.idle":"2025-02-13T06:25:40.464655Z","shell.execute_reply.started":"2025-02-13T06:24:50.594179Z","shell.execute_reply":"2025-02-13T06:25:40.463691Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport gc\nimport PIL\nimport cv2\nimport uuid\nimport shutil\nimport random\nimport glob as gb\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom PIL import Image\nfrom tqdm import tqdm  # Progress bar\nfrom scipy.special import gamma\n\nimport keras\nfrom keras.optimizers import *\nfrom keras.regularizers import l1_l2\nfrom keras.utils import to_categorical\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Input\nfrom keras.layers import GlobalAveragePooling2D\nfrom keras.callbacks import LearningRateScheduler\nfrom keras.layers import Conv2D, MaxPool2D, BatchNormalization\n\nfrom tensorflow.keras.metrics import *\nfrom tensorflow.keras.callbacks import *\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\n\n# Function to get the version of a module if it exists\ndef get_version(module):\n    try:\n        return module.__version__\n    except AttributeError:\n        return \"Version info not available\"\n\n# Print versions of each module\nprint(\"os:\", os.__version__ if hasattr(os, '__version__') else \"OS version not available\")\nprint(\"gc:\", get_version(gc))\nprint(\"PIL:\", get_version(PIL))\nprint(\"cv2:\", cv2.__version__)\nprint(\"uuid:\", get_version(uuid))\nprint(\"shutil:\", get_version(shutil))\nprint(\"random:\", get_version(random))\nprint(\"glob:\", get_version(gb))\nprint(\"numpy:\", np.__version__)\nprint(\"pandas:\", pd.__version__)\nprint(\"tensorflow:\", tf.__version__)\nimport matplotlib\nprint(\"matplotlib:\", matplotlib.__version__)\nimport tqdm\nprint(\"tqdm:\", tqdm.__version__)\nimport scipy\nprint(\"scipy:\", scipy.__version__)\nprint(\"keras:\", keras.__version__)\nprint(\"tensorflow:\", tf.__version__)\nimport sklearn\nprint(\"sklearn:\", sklearn.__version__)  # sklearn version","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:51:51.360507Z","iopub.execute_input":"2025-02-13T17:51:51.360703Z","iopub.status.idle":"2025-02-13T17:52:05.357938Z","shell.execute_reply.started":"2025-02-13T17:51:51.360683Z","shell.execute_reply":"2025-02-13T17:52:05.357002Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"path = \"/kaggle/input/cbis-ddsm-breast-cancer-image-dataset\"\n\nfor files in os.listdir(path):\n    files_dir = os.path.join(path, files)\n\n\n    if files == 'jpeg':   # to pass 6774 files \n        pass\n    else:\n        for file in os.listdir(files_dir):\n            print(file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:52:08.265337Z","iopub.execute_input":"2025-02-13T17:52:08.265649Z","iopub.status.idle":"2025-02-13T17:52:08.281085Z","shell.execute_reply.started":"2025-02-13T17:52:08.26562Z","shell.execute_reply":"2025-02-13T17:52:08.280141Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dicom_df = pd.read_csv(\"/kaggle/input/cbis-ddsm-breast-cancer-image-dataset/csv/dicom_info.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:52:08.708745Z","iopub.execute_input":"2025-02-13T17:52:08.709019Z","iopub.status.idle":"2025-02-13T17:52:08.913263Z","shell.execute_reply.started":"2025-02-13T17:52:08.708998Z","shell.execute_reply":"2025-02-13T17:52:08.912465Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dicom_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:52:08.952088Z","iopub.execute_input":"2025-02-13T17:52:08.952422Z","iopub.status.idle":"2025-02-13T17:52:09.001753Z","shell.execute_reply.started":"2025-02-13T17:52:08.952388Z","shell.execute_reply":"2025-02-13T17:52:09.00087Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dicom_df.describe().T","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:52:09.176515Z","iopub.execute_input":"2025-02-13T17:52:09.1769Z","iopub.status.idle":"2025-02-13T17:52:09.234753Z","shell.execute_reply.started":"2025-02-13T17:52:09.176867Z","shell.execute_reply":"2025-02-13T17:52:09.233827Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dicom_df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:52:27.995347Z","iopub.execute_input":"2025-02-13T17:52:27.995634Z","iopub.status.idle":"2025-02-13T17:52:28.026414Z","shell.execute_reply.started":"2025-02-13T17:52:27.995613Z","shell.execute_reply":"2025-02-13T17:52:28.025719Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dicom_df.SeriesDescription.unique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:52:33.389388Z","iopub.execute_input":"2025-02-13T17:52:33.389672Z","iopub.status.idle":"2025-02-13T17:52:33.394886Z","shell.execute_reply.started":"2025-02-13T17:52:33.38965Z","shell.execute_reply":"2025-02-13T17:52:33.394171Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dicom_df.SeriesDescription.value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:52:33.776319Z","iopub.execute_input":"2025-02-13T17:52:33.77656Z","iopub.status.idle":"2025-02-13T17:52:33.783187Z","shell.execute_reply.started":"2025-02-13T17:52:33.776541Z","shell.execute_reply":"2025-02-13T17:52:33.78244Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cropped_images = dicom_df[dicom_df.SeriesDescription==\"cropped images\"].image_path\ncropped_images.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:52:36.209721Z","iopub.execute_input":"2025-02-13T17:52:36.210058Z","iopub.status.idle":"2025-02-13T17:52:36.219292Z","shell.execute_reply.started":"2025-02-13T17:52:36.210027Z","shell.execute_reply":"2025-02-13T17:52:36.218482Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"full_mammogram = dicom_df[dicom_df.SeriesDescription==\"full mammogram images\"].image_path\nfull_mammogram.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:52:36.605796Z","iopub.execute_input":"2025-02-13T17:52:36.606101Z","iopub.status.idle":"2025-02-13T17:52:36.614541Z","shell.execute_reply.started":"2025-02-13T17:52:36.606049Z","shell.execute_reply":"2025-02-13T17:52:36.613652Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"roi_mask = dicom_df[dicom_df.SeriesDescription==\"ROI mask images\"].image_path\nroi_mask.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:52:38.748378Z","iopub.execute_input":"2025-02-13T17:52:38.748662Z","iopub.status.idle":"2025-02-13T17:52:38.75797Z","shell.execute_reply.started":"2025-02-13T17:52:38.748641Z","shell.execute_reply":"2025-02-13T17:52:38.756996Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # delete dicom_df after finished use it\ndel dicom_df;    gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:52:39.049992Z","iopub.execute_input":"2025-02-13T17:52:39.05028Z","iopub.status.idle":"2025-02-13T17:52:39.225646Z","shell.execute_reply.started":"2025-02-13T17:52:39.050257Z","shell.execute_reply":"2025-02-13T17:52:39.22487Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def replace_path(sample, old_path, new_path):\n    return sample.replace(old_path, new_path, regex=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:52:39.292259Z","iopub.execute_input":"2025-02-13T17:52:39.2925Z","iopub.status.idle":"2025-02-13T17:52:39.29598Z","shell.execute_reply.started":"2025-02-13T17:52:39.292479Z","shell.execute_reply":"2025-02-13T17:52:39.29515Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_smaples(sample, row=15, col=15):\n    plt.figure(figsize=(row, col))\n    for i, file in enumerate(sample[0:5]):\n        cropped_images_show = PIL.Image.open(file)\n        gray_img= cropped_images_show.convert(\"L\")\n        plt.subplot(1,5,i+1)\n        plt.imshow(gray_img, cmap='gray')\n        plt.axis('off')\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:52:41.660978Z","iopub.execute_input":"2025-02-13T17:52:41.661286Z","iopub.status.idle":"2025-02-13T17:52:41.665839Z","shell.execute_reply.started":"2025-02-13T17:52:41.661261Z","shell.execute_reply":"2025-02-13T17:52:41.665045Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"correct_dir = \"../input/cbis-ddsm-breast-cancer-image-dataset/jpeg\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:52:42.128348Z","iopub.execute_input":"2025-02-13T17:52:42.128618Z","iopub.status.idle":"2025-02-13T17:52:42.132163Z","shell.execute_reply.started":"2025-02-13T17:52:42.128597Z","shell.execute_reply":"2025-02-13T17:52:42.131358Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cropped_images = replace_path(cropped_images, \"CBIS-DDSM/jpeg\", correct_dir)\nprint('Cropped Images paths:')\nprint(cropped_images.iloc[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:52:43.490846Z","iopub.execute_input":"2025-02-13T17:52:43.491186Z","iopub.status.idle":"2025-02-13T17:52:43.499115Z","shell.execute_reply.started":"2025-02-13T17:52:43.491156Z","shell.execute_reply":"2025-02-13T17:52:43.498299Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_smaples(cropped_images, 15, 15)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:52:46.570148Z","iopub.execute_input":"2025-02-13T17:52:46.570435Z","iopub.status.idle":"2025-02-13T17:52:47.021338Z","shell.execute_reply.started":"2025-02-13T17:52:46.570414Z","shell.execute_reply":"2025-02-13T17:52:47.020385Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"full_mammogram = replace_path(full_mammogram, \"CBIS-DDSM/jpeg\", correct_dir)\nprint('\\nFull mammo Images paths:')\nprint(full_mammogram.iloc[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:52:49.971833Z","iopub.execute_input":"2025-02-13T17:52:49.972178Z","iopub.status.idle":"2025-02-13T17:52:49.9799Z","shell.execute_reply.started":"2025-02-13T17:52:49.972147Z","shell.execute_reply":"2025-02-13T17:52:49.978892Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_smaples(full_mammogram)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:52:51.294535Z","iopub.execute_input":"2025-02-13T17:52:51.294819Z","iopub.status.idle":"2025-02-13T17:52:55.151835Z","shell.execute_reply.started":"2025-02-13T17:52:51.294797Z","shell.execute_reply":"2025-02-13T17:52:55.150995Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"roi_mask = replace_path(roi_mask, \"CBIS-DDSM/jpeg\", correct_dir)\nprint('\\nROI Mask Images paths:')\nprint(roi_mask.iloc[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:52:55.152871Z","iopub.execute_input":"2025-02-13T17:52:55.153106Z","iopub.status.idle":"2025-02-13T17:52:55.160661Z","shell.execute_reply.started":"2025-02-13T17:52:55.153087Z","shell.execute_reply":"2025-02-13T17:52:55.159767Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_smaples(roi_mask)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:52:55.162026Z","iopub.execute_input":"2025-02-13T17:52:55.162275Z","iopub.status.idle":"2025-02-13T17:52:58.543411Z","shell.execute_reply.started":"2025-02-13T17:52:55.162254Z","shell.execute_reply":"2025-02-13T17:52:58.542589Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_image_file_name(data, new_dict):\n    \"\"\"\n    /kaggle\n    /input\n    /cbis-ddsm-breast-cancer-image-dataset\n    /jpeg\n    /1.3.6.1.4.1.9590.100.1.2.129308726812851964007517874181459556304 [5]\n    /1-172.jpg\n    \n    return path at index [5] after split depends on split('\\')\n    \"\"\"\n    for dicom in data:\n        key = dicom.split('/')[4]\n#         print(key)\n        new_dict[key] = dicom\n    print(f\"the length of dataset ==> {len(new_dict.keys())}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:52:58.544487Z","iopub.execute_input":"2025-02-13T17:52:58.544761Z","iopub.status.idle":"2025-02-13T17:52:58.54898Z","shell.execute_reply.started":"2025-02-13T17:52:58.544739Z","shell.execute_reply":"2025-02-13T17:52:58.548325Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cropped_images_dict = dict()\nfull_mammo_dict = dict()\nroi_img_dict = dict()\n\nget_image_file_name(cropped_images, cropped_images_dict)\nget_image_file_name(full_mammogram, full_mammo_dict)\nget_image_file_name(roi_mask, roi_img_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:52:58.550415Z","iopub.execute_input":"2025-02-13T17:52:58.550679Z","iopub.status.idle":"2025-02-13T17:52:58.574842Z","shell.execute_reply.started":"2025-02-13T17:52:58.550658Z","shell.execute_reply":"2025-02-13T17:52:58.574255Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"next(iter((cropped_images.items())))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:52:58.575643Z","iopub.execute_input":"2025-02-13T17:52:58.575822Z","iopub.status.idle":"2025-02-13T17:52:58.592418Z","shell.execute_reply.started":"2025-02-13T17:52:58.575806Z","shell.execute_reply":"2025-02-13T17:52:58.591655Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"next(iter((cropped_images_dict.items())))\nprint()\nprint(len(cropped_images_dict.keys()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:53:00.675635Z","iopub.execute_input":"2025-02-13T17:53:00.675925Z","iopub.status.idle":"2025-02-13T17:53:00.680666Z","shell.execute_reply.started":"2025-02-13T17:53:00.675901Z","shell.execute_reply":"2025-02-13T17:53:00.679927Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"next(iter((full_mammo_dict.items())))\nprint()\nprint(len(full_mammo_dict.keys()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:53:01.885199Z","iopub.execute_input":"2025-02-13T17:53:01.885486Z","iopub.status.idle":"2025-02-13T17:53:01.891223Z","shell.execute_reply.started":"2025-02-13T17:53:01.885464Z","shell.execute_reply":"2025-02-13T17:53:01.890054Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"next(iter((roi_img_dict.items())))\nprint()\nprint(len(roi_img_dict.keys()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:53:03.248117Z","iopub.execute_input":"2025-02-13T17:53:03.248401Z","iopub.status.idle":"2025-02-13T17:53:03.253146Z","shell.execute_reply.started":"2025-02-13T17:53:03.248378Z","shell.execute_reply":"2025-02-13T17:53:03.252354Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"del cropped_images, full_mammogram, roi_mask;    gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:53:04.503847Z","iopub.execute_input":"2025-02-13T17:53:04.504157Z","iopub.status.idle":"2025-02-13T17:53:04.698382Z","shell.execute_reply.started":"2025-02-13T17:53:04.504131Z","shell.execute_reply":"2025-02-13T17:53:04.697491Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# fix image paths\ndef fix_image_path(data):\n    \"\"\"Correct dicom paths to correct image paths.\"\"\"\n    for indx, image in enumerate(data.values):\n#         print(f\"Image Path: {image[11]}\")\n\n        img_name = image[11].split('/')[2]\n#         print(f\"Looking for key: {img_name}\")  # Debugging step\n\n        if img_name in full_mammo_dict:\n            data.iloc[indx, 11] = full_mammo_dict[img_name]\n        else:\n            data.iloc[indx, 11] = None\n#             print(f\"KeyError: '{img_name}' not found in full_mammo_dict\")  # Debugging step\n        \n        img_name = image[12].split('/')[2]\n        if img_name in cropped_images_dict:\n            data.iloc[indx, 12] = cropped_images_dict[img_name]\n        else:\n            data.iloc[indx, 11] = None\n            # print(f\"KeyError: '{img_name}' not found in cropped_images_dict\")  # Debugging step\n\n        img_name = image[13].split('/')[2]\n        if img_name in roi_img_dict:\n            data.iloc[indx, 13] = roi_img_dict[img_name]\n\n        else:\n            data.iloc[indx, 13] = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:53:06.883628Z","iopub.execute_input":"2025-02-13T17:53:06.883919Z","iopub.status.idle":"2025-02-13T17:53:06.889089Z","shell.execute_reply.started":"2025-02-13T17:53:06.883896Z","shell.execute_reply":"2025-02-13T17:53:06.888326Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mass_train = pd.read_csv(\"/kaggle/input/cbis-ddsm-breast-cancer-image-dataset/csv/mass_case_description_train_set.csv\")\nmass_test  = pd.read_csv(\"/kaggle/input/cbis-ddsm-breast-cancer-image-dataset/csv/mass_case_description_test_set.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:53:08.07447Z","iopub.execute_input":"2025-02-13T17:53:08.074748Z","iopub.status.idle":"2025-02-13T17:53:08.119677Z","shell.execute_reply.started":"2025-02-13T17:53:08.074726Z","shell.execute_reply":"2025-02-13T17:53:08.119016Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mass_train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:53:08.943256Z","iopub.execute_input":"2025-02-13T17:53:08.943532Z","iopub.status.idle":"2025-02-13T17:53:08.956412Z","shell.execute_reply.started":"2025-02-13T17:53:08.943512Z","shell.execute_reply":"2025-02-13T17:53:08.955548Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mass_train.iloc[:, 11].head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:53:10.547563Z","iopub.execute_input":"2025-02-13T17:53:10.547895Z","iopub.status.idle":"2025-02-13T17:53:10.554559Z","shell.execute_reply.started":"2025-02-13T17:53:10.547866Z","shell.execute_reply":"2025-02-13T17:53:10.55365Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fix_image_path(mass_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:53:11.777295Z","iopub.execute_input":"2025-02-13T17:53:11.77767Z","iopub.status.idle":"2025-02-13T17:53:12.410017Z","shell.execute_reply.started":"2025-02-13T17:53:11.777637Z","shell.execute_reply":"2025-02-13T17:53:12.409157Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mass_train.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:53:12.546375Z","iopub.execute_input":"2025-02-13T17:53:12.546645Z","iopub.status.idle":"2025-02-13T17:53:12.557064Z","shell.execute_reply.started":"2025-02-13T17:53:12.546621Z","shell.execute_reply":"2025-02-13T17:53:12.556201Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mass_train = mass_train.rename(columns={'left or right breast': 'left_or_right_breast',\n                                        'image view': 'image_view',\n                                        'abnormality id': 'abnormality_id',\n                                        'abnormality type': 'abnormality_type',\n                                        'mass shape': 'mass_shape',\n                                        'mass margins': 'mass_margins',\n                                        'image file path': 'image_file_path',\n                                        'cropped image file path': 'cropped_image_file_path',\n                                        'ROI mask file path': 'ROI_mask_file_path'})\nmass_train.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:53:14.197158Z","iopub.execute_input":"2025-02-13T17:53:14.197492Z","iopub.status.idle":"2025-02-13T17:53:14.211056Z","shell.execute_reply.started":"2025-02-13T17:53:14.197466Z","shell.execute_reply":"2025-02-13T17:53:14.210345Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mass_train.pathology.unique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:53:16.361856Z","iopub.execute_input":"2025-02-13T17:53:16.362162Z","iopub.status.idle":"2025-02-13T17:53:16.367859Z","shell.execute_reply.started":"2025-02-13T17:53:16.362138Z","shell.execute_reply":"2025-02-13T17:53:16.366888Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mass_test.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:53:16.861484Z","iopub.execute_input":"2025-02-13T17:53:16.86176Z","iopub.status.idle":"2025-02-13T17:53:16.874288Z","shell.execute_reply.started":"2025-02-13T17:53:16.861737Z","shell.execute_reply":"2025-02-13T17:53:16.87364Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mass_train.iloc[:, 11].head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:53:18.54058Z","iopub.execute_input":"2025-02-13T17:53:18.540877Z","iopub.status.idle":"2025-02-13T17:53:18.547034Z","shell.execute_reply.started":"2025-02-13T17:53:18.540854Z","shell.execute_reply":"2025-02-13T17:53:18.546059Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fix_image_path(mass_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:53:19.821008Z","iopub.execute_input":"2025-02-13T17:53:19.821343Z","iopub.status.idle":"2025-02-13T17:53:20.025096Z","shell.execute_reply.started":"2025-02-13T17:53:19.821318Z","shell.execute_reply":"2025-02-13T17:53:20.02425Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mass_test = mass_test.rename(columns={'left or right breast': 'left_or_right_breast',\n                                      'image view': 'image_view',\n                                      'abnormality id': 'abnormality_id',\n                                      'abnormality type': 'abnormality_type',\n                                      'mass shape': 'mass_shape',\n                                      'mass margins': 'mass_margins',\n                                      'image file path': 'image_file_path',\n                                      'cropped image file path': 'cropped_image_file_path',\n                                      'ROI mask file path': 'ROI_mask_file_path'})\n# view renamed columns\nmass_test.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:53:20.443474Z","iopub.execute_input":"2025-02-13T17:53:20.443731Z","iopub.status.idle":"2025-02-13T17:53:20.457634Z","shell.execute_reply.started":"2025-02-13T17:53:20.443711Z","shell.execute_reply":"2025-02-13T17:53:20.45689Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f'Shape of mass_train: {mass_train.shape}')\nprint(f'Shape of mass_test: {mass_test.shape}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:53:22.60427Z","iopub.execute_input":"2025-02-13T17:53:22.604558Z","iopub.status.idle":"2025-02-13T17:53:22.609023Z","shell.execute_reply.started":"2025-02-13T17:53:22.604536Z","shell.execute_reply":"2025-02-13T17:53:22.608241Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"calc_train = pd.read_csv(\"/kaggle/input/cbis-ddsm-breast-cancer-image-dataset/csv/calc_case_description_train_set.csv\")\ncalc_test  = pd.read_csv(\"/kaggle/input/cbis-ddsm-breast-cancer-image-dataset/csv/calc_case_description_test_set.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:53:23.408965Z","iopub.execute_input":"2025-02-13T17:53:23.4094Z","iopub.status.idle":"2025-02-13T17:53:23.446604Z","shell.execute_reply.started":"2025-02-13T17:53:23.409367Z","shell.execute_reply":"2025-02-13T17:53:23.445971Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"calc_train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:53:24.573039Z","iopub.execute_input":"2025-02-13T17:53:24.57334Z","iopub.status.idle":"2025-02-13T17:53:24.585062Z","shell.execute_reply.started":"2025-02-13T17:53:24.573317Z","shell.execute_reply":"2025-02-13T17:53:24.584396Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"calc_train.iloc[:,11].head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:53:25.925489Z","iopub.execute_input":"2025-02-13T17:53:25.925754Z","iopub.status.idle":"2025-02-13T17:53:25.931952Z","shell.execute_reply.started":"2025-02-13T17:53:25.925735Z","shell.execute_reply":"2025-02-13T17:53:25.931128Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"calc_train = calc_train.rename(columns={'left or right breast': 'left_or_right_breast',\n                                        'image view': 'image_view',\n                                        'abnormality id': 'abnormality_id',\n                                        'abnormality type': 'abnormality_type',\n                                        'mass shape': 'mass_shape',\n                                        'mass margins': 'mass_margins',\n                                        'image file path': 'image_file_path',\n                                        'cropped image file path': 'cropped_image_file_path',\n                                        'ROI mask file path': 'ROI_mask_file_path'})\n# view renamed columns\ncalc_train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:53:27.122836Z","iopub.execute_input":"2025-02-13T17:53:27.123156Z","iopub.status.idle":"2025-02-13T17:53:27.137434Z","shell.execute_reply.started":"2025-02-13T17:53:27.123127Z","shell.execute_reply":"2025-02-13T17:53:27.13617Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fix_image_path(calc_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:53:28.287957Z","iopub.execute_input":"2025-02-13T17:53:28.288291Z","iopub.status.idle":"2025-02-13T17:53:28.972757Z","shell.execute_reply.started":"2025-02-13T17:53:28.288264Z","shell.execute_reply":"2025-02-13T17:53:28.972149Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"calc_test.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:53:29.505284Z","iopub.execute_input":"2025-02-13T17:53:29.505548Z","iopub.status.idle":"2025-02-13T17:53:29.517116Z","shell.execute_reply.started":"2025-02-13T17:53:29.505528Z","shell.execute_reply":"2025-02-13T17:53:29.516421Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"calc_test.iloc[:,11].head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:53:31.04656Z","iopub.execute_input":"2025-02-13T17:53:31.046899Z","iopub.status.idle":"2025-02-13T17:53:31.053716Z","shell.execute_reply.started":"2025-02-13T17:53:31.04687Z","shell.execute_reply":"2025-02-13T17:53:31.052866Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"calc_test = calc_test.rename(columns={'left or right breast': 'left_or_right_breast',\n                                      'image view': 'image_view',\n                                      'abnormality id': 'abnormality_id',\n                                      'abnormality type': 'abnormality_type',\n                                      'mass shape': 'mass_shape',\n                                      'mass margins': 'mass_margins',\n                                      'image file path': 'image_file_path',\n                                      'cropped image file path': 'cropped_image_file_path',\n                                      'ROI mask file path': 'ROI_mask_file_path'})\n# view renamed columns\ncalc_test.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:53:31.707503Z","iopub.execute_input":"2025-02-13T17:53:31.707777Z","iopub.status.idle":"2025-02-13T17:53:31.720169Z","shell.execute_reply.started":"2025-02-13T17:53:31.707755Z","shell.execute_reply":"2025-02-13T17:53:31.71949Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fix_image_path(calc_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:53:33.256511Z","iopub.execute_input":"2025-02-13T17:53:33.256798Z","iopub.status.idle":"2025-02-13T17:53:33.395687Z","shell.execute_reply.started":"2025-02-13T17:53:33.256775Z","shell.execute_reply":"2025-02-13T17:53:33.395056Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f'Shape of mass_train: {calc_train.shape}')\nprint(f'Shape of mass_test: {calc_test.shape}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:53:33.654163Z","iopub.execute_input":"2025-02-13T17:53:33.654425Z","iopub.status.idle":"2025-02-13T17:53:33.658872Z","shell.execute_reply.started":"2025-02-13T17:53:33.654402Z","shell.execute_reply":"2025-02-13T17:53:33.658273Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport cv2\nimport os\nimport numpy as np\n\ndef display_images(dataset, column, number):\n    \"\"\"Displays images in dataset, handling missing files and converting formats.\"\"\"\n    \n    # create figure and axes\n    fig, axes = plt.subplots(1, number, figsize=(15, 5))\n    \n    # Loop through rows and display images\n    for index, (i, row) in enumerate(dataset.head(number).iterrows()):\n        image_path = row[column]\n        \n       # Check if image_path is valid (not None) and exists\n        if image_path is None or not os.path.exists(image_path):\n            # print(f\"File not found or invalid path: {image_path}\")\n            continue\n        \n        image = cv2.imread(image_path)\n        \n        # Handle case when image can't be read\n        if image is None:\n            # print(f\"Error reading image: {image_path}\")\n            continue\n        \n        # Convert BGR to RGB if needed (for correct color display)\n        if len(image.shape) == 3 and image.shape[2] == 3:\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        ax = axes[index]\n        ax.imshow(image, cmap='gray' if len(image.shape) == 2 else None)\n        ax.set_title(f\"{row['pathology']}\")\n        ax.axis('off')\n        print(np.array(image).shape)\n    \n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:53:34.4576Z","iopub.execute_input":"2025-02-13T17:53:34.457893Z","iopub.status.idle":"2025-02-13T17:53:34.464107Z","shell.execute_reply.started":"2025-02-13T17:53:34.457869Z","shell.execute_reply":"2025-02-13T17:53:34.463297Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('Full Mammograms:\\n')\ndisplay_images(mass_train, 'image_file_path', 5)\nprint('Cropped Mammograms:\\n')\ndisplay_images(mass_train, 'cropped_image_file_path', 5)\nprint('ROI_mask:\\n')\ndisplay_images(mass_train, 'ROI_mask_file_path', 5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:53:36.300338Z","iopub.execute_input":"2025-02-13T17:53:36.300616Z","iopub.status.idle":"2025-02-13T17:53:54.149205Z","shell.execute_reply.started":"2025-02-13T17:53:36.300596Z","shell.execute_reply":"2025-02-13T17:53:54.14843Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('Full Mammograms:\\n')\ndisplay_images(mass_test, 'image_file_path', 5)\nprint('Cropped Mammograms:\\n')\ndisplay_images(mass_test, 'cropped_image_file_path', 5)\nprint('ROI_mask:\\n')\ndisplay_images(mass_test, 'ROI_mask_file_path', 5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:53:54.150375Z","iopub.execute_input":"2025-02-13T17:53:54.150677Z","iopub.status.idle":"2025-02-13T17:54:09.956964Z","shell.execute_reply.started":"2025-02-13T17:53:54.150652Z","shell.execute_reply":"2025-02-13T17:54:09.956Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('Full Mammograms:\\n')\ndisplay_images(calc_train, 'image_file_path', 5)\nprint('Cropped Mammograms:\\n')\ndisplay_images(calc_train, 'cropped_image_file_path', 5)\nprint('ROI_mask:\\n')\ndisplay_images(calc_train, 'ROI_mask_file_path', 5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:54:19.097712Z","iopub.execute_input":"2025-02-13T17:54:19.098044Z","iopub.status.idle":"2025-02-13T17:54:36.502687Z","shell.execute_reply.started":"2025-02-13T17:54:19.098014Z","shell.execute_reply":"2025-02-13T17:54:36.501734Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('Full Mammograms:\\n')\ndisplay_images(calc_test, 'image_file_path', 5)\nprint('Cropped Mammograms:\\n')\ndisplay_images(calc_test, 'cropped_image_file_path', 5)\nprint('ROI_mask:\\n')\ndisplay_images(calc_test, 'ROI_mask_file_path', 5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:54:46.410624Z","iopub.execute_input":"2025-02-13T17:54:46.410962Z","iopub.status.idle":"2025-02-13T17:54:48.364618Z","shell.execute_reply.started":"2025-02-13T17:54:46.410925Z","shell.execute_reply":"2025-02-13T17:54:48.363706Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"full_dataset = pd.concat([calc_train, calc_test], axis=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:54:48.365822Z","iopub.execute_input":"2025-02-13T17:54:48.366156Z","iopub.status.idle":"2025-02-13T17:54:48.3713Z","shell.execute_reply.started":"2025-02-13T17:54:48.366126Z","shell.execute_reply":"2025-02-13T17:54:48.370447Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"del mass_train, mass_test, calc_train, calc_test;    gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:54:48.372669Z","iopub.execute_input":"2025-02-13T17:54:48.372955Z","iopub.status.idle":"2025-02-13T17:54:48.592663Z","shell.execute_reply.started":"2025-02-13T17:54:48.372933Z","shell.execute_reply":"2025-02-13T17:54:48.591672Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_mapper = {'MALIGNANT': 1, 'BENIGN': 0, 'BENIGN_WITHOUT_CALLBACK': 0} ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:54:57.328149Z","iopub.execute_input":"2025-02-13T17:54:57.328464Z","iopub.status.idle":"2025-02-13T17:54:57.331862Z","shell.execute_reply.started":"2025-02-13T17:54:57.328438Z","shell.execute_reply":"2025-02-13T17:54:57.331191Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"target_size = (224, 224, 3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:54:58.438514Z","iopub.execute_input":"2025-02-13T17:54:58.438803Z","iopub.status.idle":"2025-02-13T17:54:58.442287Z","shell.execute_reply.started":"2025-02-13T17:54:58.438781Z","shell.execute_reply":"2025-02-13T17:54:58.441502Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"full_dataset['labels'] = full_dataset['pathology'].replace(class_mapper).infer_objects(copy=False)\n\nfull_images = np.array(full_dataset[full_dataset[\"image_file_path\"].notna()][\"image_file_path\"].tolist())\nfull_labels = np.array(full_dataset[full_dataset[\"image_file_path\"].notna()][\"labels\"].tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:54:58.879231Z","iopub.execute_input":"2025-02-13T17:54:58.879541Z","iopub.status.idle":"2025-02-13T17:54:58.8916Z","shell.execute_reply.started":"2025-02-13T17:54:58.879515Z","shell.execute_reply":"2025-02-13T17:54:58.890727Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(full_images)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:55:00.701269Z","iopub.execute_input":"2025-02-13T17:55:00.701553Z","iopub.status.idle":"2025-02-13T17:55:00.706355Z","shell.execute_reply.started":"2025-02-13T17:55:00.701531Z","shell.execute_reply":"2025-02-13T17:55:00.705392Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"full_labels_series = pd.Series(full_labels)\n\n# Count the occurrences of each class\nlabel_counts = full_labels_series.value_counts()\n\n# Assuming 0 = benign and 1 = malignant\nbenign_count = label_counts.get(0, 0)\nmalignant_count = label_counts.get(1, 0)\n\nprint(f\"Benign images: {benign_count}\")\nprint(f\"Malignant images: {malignant_count}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:55:01.927571Z","iopub.execute_input":"2025-02-13T17:55:01.927848Z","iopub.status.idle":"2025-02-13T17:55:01.935118Z","shell.execute_reply.started":"2025-02-13T17:55:01.927827Z","shell.execute_reply":"2025-02-13T17:55:01.934088Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_classes = len(full_dataset['labels'].unique())\nnum_classes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:55:03.351674Z","iopub.execute_input":"2025-02-13T17:55:03.352Z","iopub.status.idle":"2025-02-13T17:55:03.358841Z","shell.execute_reply.started":"2025-02-13T17:55:03.351973Z","shell.execute_reply":"2025-02-13T17:55:03.358102Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_names = ['Benign', 'Malignant']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:55:03.655339Z","iopub.execute_input":"2025-02-13T17:55:03.655615Z","iopub.status.idle":"2025-02-13T17:55:03.659298Z","shell.execute_reply.started":"2025-02-13T17:55:03.655594Z","shell.execute_reply":"2025-02-13T17:55:03.658295Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dir_path = '/kaggle/working/'\n\n# # Loop through all files and subdirectories in the working directory\nfor filename in os.listdir(dir_path):\n    file_path = os.path.join(dir_path, filename)\n    try:\n        if os.path.isfile(file_path) or os.path.islink(file_path):\n            os.unlink(file_path)  # Remove the file or symbolic link\n        elif os.path.isdir(file_path):\n            shutil.rmtree(file_path)  # Remove the directory and its contents\n    except Exception as e:\n        print(f'Failed to delete {file_path}. Reason: {e}')\n\nprint(\"All files and subdirectories in '/kaggle/working/' have been removed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:55:05.038277Z","iopub.execute_input":"2025-02-13T17:55:05.038586Z","iopub.status.idle":"2025-02-13T17:55:05.044292Z","shell.execute_reply.started":"2025-02-13T17:55:05.038561Z","shell.execute_reply":"2025-02-13T17:55:05.043418Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def images_count():\n    zero_class_count = len(os.listdir(\"/kaggle/working/merged_images/0\"))\n    one_class_count  = len(os.listdir(\"/kaggle/working/merged_images/1\"))\n\n    print(f\"Number of images in class 0: {zero_class_count}\")\n    print(f\"Number of images in class 1: {one_class_count}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:55:06.391374Z","iopub.execute_input":"2025-02-13T17:55:06.391673Z","iopub.status.idle":"2025-02-13T17:55:06.395835Z","shell.execute_reply.started":"2025-02-13T17:55:06.391648Z","shell.execute_reply":"2025-02-13T17:55:06.394823Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def augment_image(image):\n    # Apply data augmentation using tf.image functions\n    image = tf.image.random_flip_left_right(image)\n#     image = tf.image.random_flip_up_down(image)\n    image = tf.image.random_brightness(image, max_delta=0.3)\n    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n    image = tf.image.random_saturation(image, lower=0.8, upper=1.2)\n    return image\n\n# Function to resize image to (224, 224, 3)\ndef resize_image(image_tensor):\n    return tf.image.resize(image_tensor, [224, 224])\n\n# Function to balance classes by augmenting images\ndef copy_images_with_unique_filenames(images, labels, source, destination, target_count=None):\n    \"\"\"\n    Copy images from source to destination in subfolders '0' and '1',\n    ensuring unique filenames and applying data augmentation and balancing.\n    \"\"\"\n    benign_images = 0\n    malignant_images = 0\n    skipped_images = []\n\n    # Create the destination subfolders '0' and '1'\n    category_dest_dir_zero = os.path.join(destination, '0')\n    os.makedirs(category_dest_dir_zero, exist_ok=True)\n\n    category_dest_dir_one = os.path.join(destination, '1')\n    os.makedirs(category_dest_dir_one, exist_ok=True)\n\n    benign_images_list = []\n    malignant_images_list = []\n\n    for i, (image, label) in enumerate(zip(images, labels)):\n#         img_name = data_frame.REFNUM[i]\n#         abs_path = os.path.join(source, img_name + '.pgm')\n\n        if os.path.exists(image):\n            try:\n                # Generate a unique filename\n                filename = os.path.basename(image)\n                unique_filename = f\"{uuid.uuid4().hex}_{filename}\"\n        \n                # Open the image using PIL\n                with Image.open(image) as img:\n                    # Convert the image to RGB mode (for saving as JPEG)\n                    img = img.convert('RGB')\n                    # Augment the image (convert it to a Tensor first)\n                    img_tensor = tf.convert_to_tensor(img)\n                    # Resize the image to (224, 224, 3)\n                    resized_img_tensor = resize_image(img_tensor)\n                    augmented_image_tensor = augment_image(resized_img_tensor)\n                    # Convert Tensor back to PIL image for saving\n                    augmented_image = tf.keras.preprocessing.image.array_to_img(augmented_image_tensor)\n\n                    if label == 0:\n                        benign_images_list.append(unique_filename)\n                        dest_path = os.path.join(category_dest_dir_zero, unique_filename)\n#                         augmented_image.save(dest_path, 'JPEG')\n                        augmented_image.save(dest_path, 'JPEG')\n                        benign_images += 1\n\n                    elif label == 1:\n                        malignant_images_list.append(unique_filename)\n                        dest_path = os.path.join(category_dest_dir_one, unique_filename)\n#                         augmented_image.save(dest_path, 'JPEG')\n                        augmented_image.save(dest_path, 'JPEG')\n                        malignant_images += 1\n                        \n#                 del img, img_tensor, resized_img_tensor, augmented_image_tensor, augmented_image\n#                 gc.collect()\n            except Exception as e:\n                print(f\"Error copying image {image}: {e}\")\n                skipped_images.append(image)\n        else:\n            print(f\"Image not found: {image}\")\n            skipped_images.append(image)\n\n    # If balancing is needed, duplicate/augment images from the smaller class\n    benign_count = len(benign_images_list)\n    malignant_count = len(malignant_images_list)\n\n    if benign_count < malignant_count:\n#         augment_and_save_images(benign_images_list, category_dest_dir_zero, target_count - benign_count)\n        augment_and_save_images(benign_images_list, category_dest_dir_zero, malignant_count - benign_count)\n\n    elif malignant_count < benign_count:\n        augment_and_save_images(malignant_images_list, category_dest_dir_one, benign_count - malignant_count)\n    print(\"data balancing\")\n    images_count()\n    augment_and_save_images(benign_images_list, category_dest_dir_zero, target_count)\n    augment_and_save_images(malignant_images_list, category_dest_dir_one, target_count)\n    print(\"data augmentation\")\n    images_count()\n    \n    print(f\"\\nCopying complete.\")\n    print(f\"Benign images copied (label 0): {benign_images}\")\n    print(f\"Benign count (label 0): {benign_count}\")\n    print(f\"Malignant images copied (label 1): {malignant_images}\")\n    print(f\"Malignant count (label 1): {malignant_count}\")\n    print(f\"Total skipped images: {len(skipped_images)}\")\n    if skipped_images:\n        print(\"Skipped images:\")\n        for img in skipped_images:\n            print(img)\n            \n    del skipped_images, benign_images_list, malignant_images_list\n    gc.collect()\n\n# Function to augment and save images to balance the dataset\ndef augment_and_save_images(images_list, destination_dir, num_augments):\n    \"\"\"\n    Augment and save images to balance the dataset.\n    \"\"\"\n    for i in range(num_augments):\n        img_name = random.choice(images_list)\n        abs_path = os.path.join(destination_dir, img_name)\n\n        try:\n            with Image.open(abs_path) as img:\n                img = img.convert('RGB')\n                # Augment the image\n                img_tensor = tf.convert_to_tensor(img)\n                # Resize the image\n#                 resized_img_tensor = resize_image(img_tensor)\n                augmented_image_tensor = augment_image(img_tensor)\n                # Convert Tensor back to PIL image for saving\n                augmented_image = tf.keras.preprocessing.image.array_to_img(augmented_image_tensor)\n                # Remove the original extension from img_name 1-285.jpg --> 1-285\n                img_name_without_ext = os.path.splitext(img_name)[0]\n                # Save augmented image with a unique name\n                augmented_image.save(os.path.join(destination_dir, img_name_without_ext + f'_aug{i}.jpg'), 'JPEG')\n            \n#                 del img, img_tensor, augmented_image_tensor, augmented_image, img_name_without_ext\n#                 gc.collect()\n        except Exception as e:\n            print(f\"Error augmenting image {abs_path}: {e}\")\n\n# Example usage\nsource_dir = \"/kaggle/input/mias-mammography/all-mias\"\ndestination_dir = \"/kaggle/working/merged_images\"\n\n# target_count=0 meaning no Augmentation, There's just Data-Balance\ntarget_count = int((len(full_labels) * 5) // 2)\ncopy_images_with_unique_filenames(full_images, full_labels, source_dir, destination_dir, target_count)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:55:07.931326Z","iopub.execute_input":"2025-02-13T17:55:07.931631Z","iopub.status.idle":"2025-02-13T18:03:51.539112Z","shell.execute_reply.started":"2025-02-13T17:55:07.931605Z","shell.execute_reply":"2025-02-13T18:03:51.538286Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"target_count","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T18:03:51.540296Z","iopub.execute_input":"2025-02-13T18:03:51.540572Z","iopub.status.idle":"2025-02-13T18:03:51.545107Z","shell.execute_reply.started":"2025-02-13T18:03:51.540552Z","shell.execute_reply":"2025-02-13T18:03:51.544461Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images_count()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T18:04:16.389375Z","iopub.execute_input":"2025-02-13T18:04:16.389682Z","iopub.status.idle":"2025-02-13T18:04:16.401501Z","shell.execute_reply.started":"2025-02-13T18:04:16.389659Z","shell.execute_reply":"2025-02-13T18:04:16.400628Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\ndata_dir = '/kaggle/working/merged_images'  # Update with your dataset path\n\n# Create a dataset for the entire data to use for split\nfull_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n    data_dir,\n    labels='inferred',\n    label_mode='categorical',\n    # image_size=(224, 224),\n    image_size=(224, 224),\n    seed=30,\n    shuffle=True,\n    batch_size=13  # Set your desired batch size\n)\n# Calculate the total number of samples\ntotal_samples = tf.data.experimental.cardinality(full_dataset).numpy()\n\n# Step 3: Split the dataset into train, validation, and test sets\ntrain_size = int(0.7 * total_samples)                 # 70% for training\n# val_size   = int(0.15 * total_samples)                # 20% for validation\n# test_size = total_samples - train_size - val_size     # 10% for testing\ntest_size = total_samples - train_size                # 30% for testing\n\n# Create train, validation, and test datasets\ntrain_dataset       = full_dataset.take(train_size)\n# validation_dataset  = full_dataset.skip(train_size).take(val_size)\n# test_dataset        = full_dataset.skip(train_size + val_size)\ntest_dataset        = full_dataset.skip(train_size)\n\n# Step 4: Apply Augmentation to the Training Dataset\n# train_dataset = train_dataset.map(\n#     lambda x, y: (data_augmentation(x, training=True), y),\n#     num_parallel_calls=tf.data.experimental.AUTOTUNE\n# )\n\n# Step 5: Prefetch to Improve Performance\ntrain_dataset      = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n# validation_dataset = validation_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\ntest_dataset       = test_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n\n# Print the number of samples in each dataset\nprint(f\"Train samples:      {train_size}     batches(8) ==> {train_size*13}\")\n# print(f\"Validation samples: {val_size}       batches(13) ==> {val_size*13}\")\nprint(f\"Test samples:       {test_size}      batches(8) ==> {test_size*13}\")\n\n# del full_dataset, total_samples, train_size, val_size, test_size;    gc.collect()\ndel full_dataset, total_samples, train_size, test_size;    gc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T18:04:22.640805Z","iopub.execute_input":"2025-02-13T18:04:22.641254Z","iopub.status.idle":"2025-02-13T18:04:25.323593Z","shell.execute_reply.started":"2025-02-13T18:04:22.641216Z","shell.execute_reply":"2025-02-13T18:04:25.322749Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.applications import (ResNet50,\n                                           Xception,\n                                           VGG19,\n                                           InceptionV3,\n                                           DenseNet121,\n                                           NASNetMobile,\n                                           MobileNetV2,\n                                           MobileNet,\n                                           EfficientNetV2B0,\n                                           EfficientNetV2S,\n                                           EfficientNetV2L,\n                                           ConvNeXtBase)\n\ndef create_model(model_name, trainable_layers, dropout_value, save_weights=False):\n    model_dict = {\n        \"ResNet50\":         ResNet50,\n        \"Xception\":         Xception,\n        \"VGG19\":         VGG19,\n        \"InceptionV3\":      InceptionV3,\n        \"DenseNet121\":      DenseNet121,\n        \"NASNetMobile\":     NASNetMobile,\n        \"MobileNetV2\":      MobileNetV2,\n        \"MobileNet\":        MobileNet,\n        \"EfficientNetV2B0\": EfficientNetV2B0,\n        \"EfficientNetV2S\":  EfficientNetV2S,\n        \"EfficientNetV2L\":  EfficientNetV2L,\n        \"ConvNeXtBase\":     ConvNeXtBase,\n    }\n    \n    if model_name not in model_dict:\n        raise ValueError(f\"Model {model_name} is not supported.\")\n\n    # Load the base model without the top layers\n    base_model = model_dict[model_name](weights='imagenet' if not save_weights else None,\n                                        include_top=False,\n                                        input_shape=(224, 224, 3))\n\n    # Freeze all layers initially\n    for layer in base_model.layers:\n        layer.trainable = False\n\n    # Calculate the index to start unfreezing layers\n    from_index = int(np.round((len(base_model.layers) - 1) * (1.0 - trainable_layers / 100.0)))\n\n    # Unfreeze layers from the calculated index onwards\n    for layer in base_model.layers[from_index:]:\n        layer.trainable = True\n\n    # Add custom layers on top (Upper-Layers)\n    x = base_model.output\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(1024, activation='relu')(x)\n    x = Dropout(dropout_value)(x)\n    predictions = Dense(2, activation='softmax')(x)  # Assuming binary classification\n\n    model = Model(inputs=base_model.input, outputs=predictions)\n    \n    # Clear the base model from memory if needed (optional)\n    del model_dict, base_model, from_index, x, predictions;    gc.collect()\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T18:04:31.867513Z","iopub.execute_input":"2025-02-13T18:04:31.867802Z","iopub.status.idle":"2025-02-13T18:04:31.881096Z","shell.execute_reply.started":"2025-02-13T18:04:31.867779Z","shell.execute_reply":"2025-02-13T18:04:31.880111Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from multiprocessing import cpu_count\nn_cores = cpu_count()\nprint(f'Number of Logical CPU cores: {n_cores}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T18:04:38.579819Z","iopub.execute_input":"2025-02-13T18:04:38.580145Z","iopub.status.idle":"2025-02-13T18:04:38.584856Z","shell.execute_reply.started":"2025-02-13T18:04:38.580121Z","shell.execute_reply":"2025-02-13T18:04:38.583946Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def make_file():\n    os.makedirs(CHECKPOINTS_DIR, exist_ok=True)\n    os.makedirs(LOGS_DIR, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T18:04:38.904682Z","iopub.execute_input":"2025-02-13T18:04:38.905091Z","iopub.status.idle":"2025-02-13T18:04:38.908823Z","shell.execute_reply.started":"2025-02-13T18:04:38.90504Z","shell.execute_reply":"2025-02-13T18:04:38.907992Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from keras import backend as K\n\ndef reset_memory():\n    K.clear_session()\n    gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T18:04:41.04163Z","iopub.execute_input":"2025-02-13T18:04:41.041918Z","iopub.status.idle":"2025-02-13T18:04:41.045804Z","shell.execute_reply.started":"2025-02-13T18:04:41.041896Z","shell.execute_reply":"2025-02-13T18:04:41.044942Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"PROJECT_PATH    = \"/kaggle/working/\"\nCHECKPOINTS_DIR = os.path.join(PROJECT_PATH, \"Checkpoints\")\nLOGS_DIR        = os.path.join(PROJECT_PATH, \"Logs\")\n    \n# Define the width you want for centering\nWIDTH = 162","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T18:04:42.987592Z","iopub.execute_input":"2025-02-13T18:04:42.987878Z","iopub.status.idle":"2025-02-13T18:04:42.991988Z","shell.execute_reply.started":"2025-02-13T18:04:42.987856Z","shell.execute_reply":"2025-02-13T18:04:42.991035Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def run_model(trainable_layers,\n              optimizer_class,\n              batch_size_value,\n              dropout_value,\n              model_name=None,\n              save=False,\n              epochs=5,\n              patience=2):\n    \n    if model_name is None:\n        models = [\"ConvNeXtBase\"]\n        # models = [\"ResNet50\"]\n    else:\n        models = [model_name]  # Train only the specific model passed in\n\n    best_score = float('-inf')  # Initialize best score to negative infinity\n    best_model_info = None      # To store the best model information\n\n    for model_name in models:\n        # Clear any existing Keras session to free up memory\n        tf.keras.backend.clear_session();    gc.collect()  # Optional: use garbage collection to ensure no memory leaks\n\n        model = create_model(model_name, trainable_layers, dropout_value)  # Create the model\n        optimizer_value = optimizer_class()  # Instantiate the optimizer\n\n        model.compile(optimizer=optimizer_value,\n                      loss='categorical_crossentropy',\n                      metrics=['accuracy', Precision(name='precision'), Recall(name='recall')])  # Compile the model\n\n        trainable_params = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\n        print('')\n        output = (f\"Training model: {model_name}   \"\n                  f\"Layers: {len(model.layers)}   \"\n                  f\"Parameters: {model.count_params()}   \"\n                  f\"Trainable parameters: {trainable_params}\")\n        print(output.center(WIDTH))\n        print(f'#%% {\"-\" * 97} %'.center(WIDTH))\n\n        # Prepare file paths if saving is enabled\n        if save:\n            solution = [trainable_layers, optimizer_value.__class__.__name__, batch_size_value, dropout_value]\n            keyword = f\"{model_name}-\" + \"-\".join([str(el) for el in solution])\n            checkpointPath = os.path.join(PROJECT_PATH, \"Checkpoints\", keyword) + \".weights.h5\"\n            csvLogPath = os.path.join(PROJECT_PATH, \"Logs\", keyword) + \".csv\"\n            make_file()  # Create directories\n\n        # Set common callbacks\n        callbacks = [\n            TerminateOnNaN(),\n            EarlyStopping(monitor=\"accuracy\", mode=\"max\", patience=patience)\n        ]\n\n        # Add saving-related callbacks if saving is enabled\n        if save:\n            callbacks += [\n                ModelCheckpoint(checkpointPath,\n                                save_best_only=True,\n                                save_weights_only=True,\n                                monitor=\"accuracy\",\n                                mode=\"max\",\n                                verbose=0),\n                CSVLogger(csvLogPath, append=True)\n            ]\n\n        # Train the model\n        history = model.fit(\n            train_dataset,\n            # validation_data=validation_dataset,\n            batch_size=batch_size_value,\n            epochs=epochs,\n            callbacks=callbacks\n        )\n        \n        if save:\n            return model, history\n        \n        score = history.history['accuracy'][-1]\n        # score = (val_acc + val_precision + val_recall) / 3  # Average score\n\n        # Check if the current score is better than the best score\n        if score > best_score:\n            best_score = score  # Update best score\n            best_model_info = {\n                'model_name': model_name,\n                'trainable_layers': trainable_layers,\n                'optimizer': optimizer_value.__class__.__name__,\n                'batch_size': batch_size_value,\n                'dropout_value': dropout_value\n            }\n\n        # Clear the session after model is trained\n        reset_memory()\n        print(f\"Model {model_name} cleared from memory\".center(WIDTH))\n        print()\n\n\n    # return best_score, best_model_info  # Return best score and details of best model\n        return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T18:04:47.298969Z","iopub.execute_input":"2025-02-13T18:04:47.299321Z","iopub.status.idle":"2025-02-13T18:04:47.308235Z","shell.execute_reply.started":"2025-02-13T18:04:47.299292Z","shell.execute_reply":"2025-02-13T18:04:47.307513Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model, history = run_model(5,\n                           Adam,\n                           8,\n                           0,\n                           None,\n                           True,\n                           10,\n                           # save=True,\n                           patience=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T18:04:50.780625Z","iopub.execute_input":"2025-02-13T18:04:50.780904Z","iopub.status.idle":"2025-02-13T18:17:02.232041Z","shell.execute_reply.started":"2025-02-13T18:04:50.780884Z","shell.execute_reply":"2025-02-13T18:17:02.231159Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import model_from_json, save_model, load_model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport shutil  # For creating zip files for download\n\n# Save the model architecture to a JSON file\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model.to_json())\n\n\n# To save the entire model (architecture and weights) to a single HDF5 file\nmodel.save(\"entire_model.h5\")\n\nshutil.make_archive('entire_model', 'zip', '.', 'entire_model.h5')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T18:18:06.455969Z","iopub.execute_input":"2025-02-13T18:18:06.456337Z","iopub.status.idle":"2025-02-13T18:18:31.421395Z","shell.execute_reply.started":"2025-02-13T18:18:06.456307Z","shell.execute_reply":"2025-02-13T18:18:31.420577Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink('entire_model.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T18:19:19.530343Z","iopub.execute_input":"2025-02-13T18:19:19.530629Z","iopub.status.idle":"2025-02-13T18:19:19.535678Z","shell.execute_reply.started":"2025-02-13T18:19:19.530606Z","shell.execute_reply":"2025-02-13T18:19:19.534994Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.utils import plot_model\n\nplot_model(model, to_file=\"model_architecture.png\", show_shapes=True, show_layer_names=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T18:19:51.072877Z","iopub.execute_input":"2025-02-13T18:19:51.073238Z","iopub.status.idle":"2025-02-13T18:19:54.263162Z","shell.execute_reply.started":"2025-02-13T18:19:51.073208Z","shell.execute_reply":"2025-02-13T18:19:54.261833Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.utils import plot_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T18:20:33.276497Z","iopub.execute_input":"2025-02-13T18:20:33.276815Z","iopub.status.idle":"2025-02-13T18:20:33.280672Z","shell.execute_reply.started":"2025-02-13T18:20:33.27679Z","shell.execute_reply":"2025-02-13T18:20:33.279659Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_dir = '/kaggle/working/merged_images/0'\nimage_path = os.path.join(image_dir, os.listdir(image_dir)[0])\n\n# Read the image\nimage = cv2.imread(image_path)\n\n# Resize the image to the desired shape\nimage = cv2.resize(image, (224, 224))  # OpenCV uses (width, height) format for resizing\n\n# Convert the image to a NumPy array and preprocess it for the model\nimage = np.array(image)\nprint(image.shape)\n\n# Optionally, if your model expects the image in a specific format (e.g., scaled to [0, 1]),\n# you may need to normalize it as follows:\nimage = image / 255.0  # Scale pixel values to [0, 1]\n\n# Add a batch dimension (1, 224, 224, 3)\nimage = np.expand_dims(image, axis=0)\n\n# Display the image (optional)\nplt.imshow(image[0])  # Use image[0] to display the image without the batch dimension\nplt.axis('off')  # Turn off axis labels\nplt.show()\n\n# Make predictions using the new model\nresult = model.predict(image)\nprint(\"Prediction Result:\", result)\n\ndel image;    gc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T18:20:33.584835Z","iopub.execute_input":"2025-02-13T18:20:33.585148Z","iopub.status.idle":"2025-02-13T18:20:38.894205Z","shell.execute_reply.started":"2025-02-13T18:20:33.585123Z","shell.execute_reply":"2025-02-13T18:20:38.893295Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# Specify the directory\nworking_dir = '/kaggle/working/'\n\n# Iterate through files in the directory\nfor file_name in os.listdir(working_dir):\n    # Check if the file is a .png file\n    if file_name.endswith('.png'):\n        # Construct full file path\n        file_path = os.path.join(working_dir, file_name)\n        # Remove the file\n        os.remove(file_path)\n        print(f\"Deleted: {file_path}\")\n\nprint(\"All .png images have been removed from the working directory.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T18:20:43.301578Z","iopub.execute_input":"2025-02-13T18:20:43.301879Z","iopub.status.idle":"2025-02-13T18:20:43.307711Z","shell.execute_reply.started":"2025-02-13T18:20:43.301856Z","shell.execute_reply":"2025-02-13T18:20:43.306883Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epochs = range(len(history.history['accuracy']))\ntrain_accuracy = history.history['accuracy']\ntrain_precision = history.history['precision']  # Ensure these are recorded in history\ntrain_recall = history.history['recall']\ntrain_loss = history.history['loss']\n\n# Accuracy Plot\nplt.figure(figsize=(8, 5))\nplt.plot(epochs, train_accuracy, label='Train Accuracy', color='blue', marker='o')\nplt.title('Training Accuracy Over Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.grid()\n\n# Annotate training accuracy values\nfor i, acc in enumerate(train_accuracy):\n    plt.text(i, acc + 0.01, f\"{acc:.4f}\", ha='center')\n\nplt.tight_layout()\nplt.savefig('training_accuracy.png')\nplt.show()\n\n# Precision Plot\nplt.figure(figsize=(8, 5))\nplt.plot(epochs, train_precision, label='Train Precision', color='purple', marker='o')\nplt.title('Training Precision Over Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Precision')\nplt.grid()\n\n# Annotate training precision values\nfor i, prec in enumerate(train_precision):\n    plt.text(i, prec + 0.01, f\"{prec:.4f}\", ha='center')\n\nplt.tight_layout()\nplt.savefig('training_precision.png')\nplt.show()\n\n# Recall Plot\nplt.figure(figsize=(8, 5))\nplt.plot(epochs, train_recall, label='Train Recall', color='orange', marker='o')\nplt.title('Training Recall Over Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Recall')\nplt.grid()\n\n# Annotate training recall values\nfor i, rec in enumerate(train_recall):\n    plt.text(i, rec + 0.01, f\"{rec:.4f}\", ha='center')\n\nplt.tight_layout()\nplt.savefig('training_recall.png')\nplt.show()\n\n# Loss Plot\nplt.figure(figsize=(8, 5))\nplt.plot(epochs, train_loss, label='Train Loss', color='red', marker='o')\nplt.title('Training Loss Over Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.grid()\n\n# Annotate training loss values\nfor i, loss in enumerate(train_loss):\n    plt.text(i, loss + 0.01, f\"{loss:.4f}\", ha='center')\n\nplt.tight_layout()\nplt.savefig('training_loss.png')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T18:20:43.584212Z","iopub.execute_input":"2025-02-13T18:20:43.584506Z","iopub.status.idle":"2025-02-13T18:20:44.891932Z","shell.execute_reply.started":"2025-02-13T18:20:43.584483Z","shell.execute_reply":"2025-02-13T18:20:44.891153Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_results = model.evaluate(test_dataset)\nprint(test_results)\ntest_loss = test_results[0]\ntest_accuracy = test_results[1]\ntest_precision = test_results[2]\ntest_recall = test_results[3]\n\n# Test metrics for bar plot\ntest_metrics = ['Accuracy', 'Precision', 'Recall', 'Loss']\ntest_values = [test_accuracy, test_precision, test_recall, test_loss]\n\n# Plot test metrics as bars\nplt.figure(figsize=(8, 5))\n\n# Bar plot with custom colors\nbars = plt.bar(test_metrics, test_values, color=['#003049', '#219ebc', '#f77f00', '#d62828'])\n\n# Set y-axis limits and labels\nplt.ylim(0, 1)  # Adjust for metric ranges (may vary)\nplt.title('Test Metrics', fontsize=14)\nplt.ylabel('Value', fontsize=12)\n\n# Annotate each bar with the corresponding value\nfor bar in bars:\n    yval = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width() / 2, yval + 0.02, f'{yval:.4f}', ha='center', va='bottom', fontsize=10)\n\n# Tight layout for better spacing\nplt.tight_layout()\n\n# Save and display the plot\nplt.savefig('test_metrics_bar_custom_colors.png')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T18:20:44.89298Z","iopub.execute_input":"2025-02-13T18:20:44.893314Z","iopub.status.idle":"2025-02-13T18:21:27.395125Z","shell.execute_reply.started":"2025-02-13T18:20:44.893289Z","shell.execute_reply":"2025-02-13T18:21:27.39416Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n# Get true labels and predicted labels for the test data\ny_true = np.concatenate([y for x, y in test_dataset], axis=0).argmax(axis=1)\ny_pred = np.concatenate([model.predict(x) for x, y in test_dataset], axis=0).argmax(axis=1)\n\n# Generate the confusion matrix\nconf_matrix = confusion_matrix(y_true, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)\n\n# Plot the confusion matrix\nplt.figure(figsize=(8, 6))\ndisp.plot(cmap='Blues')\nplt.title('Confusion Matrix')\nplt.grid(False)\nplt.tight_layout()\nplt.savefig('confusion_matrix.png')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T18:21:27.396616Z","iopub.execute_input":"2025-02-13T18:21:27.397147Z","iopub.status.idle":"2025-02-13T18:22:20.695886Z","shell.execute_reply.started":"2025-02-13T18:21:27.39712Z","shell.execute_reply":"2025-02-13T18:22:20.694897Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}