{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1873742,"sourceType":"datasetVersion","datasetId":1115384}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport PIL\nimport cv2\nimport uuid\nimport shutil\nimport random\nimport glob as gb\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom PIL import Image\nfrom tqdm import tqdm  # Progress bar\nfrom scipy.special import gamma\n\nfrom keras.optimizers import *\nfrom keras.regularizers import l1_l2\nfrom keras.utils import to_categorical\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Input\nfrom keras.layers import GlobalAveragePooling2D\nfrom keras.callbacks import LearningRateScheduler\nfrom keras.layers import Conv2D, MaxPool2D, BatchNormalization\n\nfrom tensorflow.keras.metrics import *\nfrom tensorflow.keras.callbacks import *\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:11:50.46047Z","iopub.execute_input":"2024-12-17T05:11:50.460825Z","iopub.status.idle":"2024-12-17T05:11:50.469126Z","shell.execute_reply.started":"2024-12-17T05:11:50.460796Z","shell.execute_reply":"2024-12-17T05:11:50.468207Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"calc_train = pd.read_csv('/kaggle/input/cbis-ddsm-breast-cancer-image-dataset/csv/calc_case_description_train_set.csv')\ncalc_test = pd.read_csv('/kaggle/input/cbis-ddsm-breast-cancer-image-dataset/csv/calc_case_description_test_set.csv')\nmass_train = pd.read_csv('/kaggle/input/cbis-ddsm-breast-cancer-image-dataset/csv/mass_case_description_train_set.csv')\nmass_test = pd.read_csv('/kaggle/input/cbis-ddsm-breast-cancer-image-dataset/csv/mass_case_description_test_set.csv')\ndicom_df = pd.read_csv('/kaggle/input/cbis-ddsm-breast-cancer-image-dataset/csv/dicom_info.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:11:53.628056Z","iopub.execute_input":"2024-12-17T05:11:53.628381Z","iopub.status.idle":"2024-12-17T05:11:53.787783Z","shell.execute_reply.started":"2024-12-17T05:11:53.628353Z","shell.execute_reply":"2024-12-17T05:11:53.787052Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def replace_path(sample, old_path, new_path):\n    return sample.replace(old_path, new_path, regex=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:11:59.000703Z","iopub.execute_input":"2024-12-17T05:11:59.001306Z","iopub.status.idle":"2024-12-17T05:11:59.0053Z","shell.execute_reply.started":"2024-12-17T05:11:59.001275Z","shell.execute_reply":"2024-12-17T05:11:59.004378Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_smaples(sample, row=15, col=15):\n    plt.figure(figsize=(row, col))\n    for i, file in enumerate(sample[0:5]):\n        cropped_images_show = PIL.Image.open(file)\n        gray_img= cropped_images_show.convert(\"L\")\n        plt.subplot(1,5,i+1)\n        plt.imshow(gray_img, cmap='gray')\n        plt.axis('off')\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:12:06.522263Z","iopub.execute_input":"2024-12-17T05:12:06.522915Z","iopub.status.idle":"2024-12-17T05:12:06.527753Z","shell.execute_reply.started":"2024-12-17T05:12:06.522881Z","shell.execute_reply":"2024-12-17T05:12:06.526837Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cropped_images = dicom_df[dicom_df.SeriesDescription==\"cropped images\"].image_path\nfull_mammogram = dicom_df[dicom_df.SeriesDescription==\"full mammogram images\"].image_path\nroi_mask = dicom_df[dicom_df.SeriesDescription==\"ROI mask images\"].image_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:12:09.209108Z","iopub.execute_input":"2024-12-17T05:12:09.209439Z","iopub.status.idle":"2024-12-17T05:12:09.226301Z","shell.execute_reply.started":"2024-12-17T05:12:09.209409Z","shell.execute_reply":"2024-12-17T05:12:09.225537Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Replace the path for cropped_images to the correct directory.\ncorrect_dir = \"../input/cbis-ddsm-breast-cancer-image-dataset/jpeg\"\ncropped_images = replace_path(cropped_images, \"CBIS-DDSM/jpeg\", correct_dir)\nprint('Cropped Images paths:')\nprint(cropped_images.iloc[0]) # Print to ensure everything looks correct.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:12:13.17847Z","iopub.execute_input":"2024-12-17T05:12:13.179146Z","iopub.status.idle":"2024-12-17T05:12:13.187599Z","shell.execute_reply.started":"2024-12-17T05:12:13.179113Z","shell.execute_reply":"2024-12-17T05:12:13.186682Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Replace the path for full_mammogram images to the correct directory.\nfull_mammogram = replace_path(full_mammogram, \"CBIS-DDSM/jpeg\", correct_dir)\nprint('\\nFull mammo Images paths:')\nprint(full_mammogram.iloc[0]) # Print to ensure everything looks correct.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:12:16.76707Z","iopub.execute_input":"2024-12-17T05:12:16.767392Z","iopub.status.idle":"2024-12-17T05:12:16.775013Z","shell.execute_reply.started":"2024-12-17T05:12:16.767365Z","shell.execute_reply":"2024-12-17T05:12:16.774082Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Replace the path for roi_mask images to the correct directory.\nroi_mask = replace_path(roi_mask, \"CBIS-DDSM/jpeg\", correct_dir)\nprint('\\nROI Mask Images paths:')\nprint(roi_mask.iloc[0]) # Print to ensure everything looks correct.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:12:18.810213Z","iopub.execute_input":"2024-12-17T05:12:18.810914Z","iopub.status.idle":"2024-12-17T05:12:18.818066Z","shell.execute_reply.started":"2024-12-17T05:12:18.81088Z","shell.execute_reply":"2024-12-17T05:12:18.817137Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here we are creating a \"get_image_file_name\" function to find the length of each dataset, and ensure this matches and all pictures are implemented as expected.","metadata":{}},{"cell_type":"code","source":"def get_image_file_name(data, new_dict):\n\n    for dicom in data:\n        key = dicom.split('/')[4]\n        new_dict[key] = dicom\n    print(f\"the length of dataset ==> {len(new_dict.keys())}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:12:24.61859Z","iopub.execute_input":"2024-12-17T05:12:24.618943Z","iopub.status.idle":"2024-12-17T05:12:24.623285Z","shell.execute_reply.started":"2024-12-17T05:12:24.618916Z","shell.execute_reply":"2024-12-17T05:12:24.622525Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cropped_images_dict = dict()\nfull_mammo_dict = dict()\nroi_img_dict = dict()\n\nget_image_file_name(cropped_images, cropped_images_dict)\nget_image_file_name(full_mammogram, full_mammo_dict)\nget_image_file_name(roi_mask, roi_img_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:12:26.434789Z","iopub.execute_input":"2024-12-17T05:12:26.435434Z","iopub.status.idle":"2024-12-17T05:12:26.445468Z","shell.execute_reply.started":"2024-12-17T05:12:26.4354Z","shell.execute_reply":"2024-12-17T05:12:26.444577Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def fix_image_path(data):\n    \"\"\"Correct dicom paths to correct image paths.\"\"\"\n    for indx, image in enumerate(data.values):\n\n        img_name = image[11].split('/')[2]\n\n        if img_name in full_mammo_dict:\n            data.iloc[indx, 11] = full_mammo_dict[img_name]\n        else:\n            data.iloc[indx, 11] = None\n        \n        img_name = image[12].split('/')[2]\n        if img_name in cropped_images_dict:\n            data.iloc[indx, 12] = cropped_images_dict[img_name]\n        else:\n            data.iloc[indx, 11] = None\n\n        img_name = image[13].split('/')[2]\n        if img_name in roi_img_dict:\n            data.iloc[indx, 13] = roi_img_dict[img_name]\n\n        else:\n            data.iloc[indx, 13] = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:12:28.084853Z","iopub.execute_input":"2024-12-17T05:12:28.085175Z","iopub.status.idle":"2024-12-17T05:12:28.091203Z","shell.execute_reply.started":"2024-12-17T05:12:28.085149Z","shell.execute_reply":"2024-12-17T05:12:28.090362Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fix_image_path(mass_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:12:30.007292Z","iopub.execute_input":"2024-12-17T05:12:30.008115Z","iopub.status.idle":"2024-12-17T05:12:30.550373Z","shell.execute_reply.started":"2024-12-17T05:12:30.008079Z","shell.execute_reply":"2024-12-17T05:12:30.549697Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mass_train = mass_train.rename(columns={'left or right breast': 'left_or_right_breast',\n                                        'image view': 'image_view',\n                                        'abnormality id': 'abnormality_id',\n                                        'abnormality type': 'abnormality_type',\n                                        'mass shape': 'mass_shape',\n                                        'mass margins': 'mass_margins',\n                                        'image file path': 'image_file_path',\n                                        'cropped image file path': 'cropped_image_file_path',\n                                        'ROI mask file path': 'ROI_mask_file_path'})\nmass_train.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:12:31.304106Z","iopub.execute_input":"2024-12-17T05:12:31.304515Z","iopub.status.idle":"2024-12-17T05:12:31.32317Z","shell.execute_reply.started":"2024-12-17T05:12:31.304478Z","shell.execute_reply":"2024-12-17T05:12:31.322435Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mass_train.pathology.unique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:12:32.458717Z","iopub.execute_input":"2024-12-17T05:12:32.45904Z","iopub.status.idle":"2024-12-17T05:12:32.467863Z","shell.execute_reply.started":"2024-12-17T05:12:32.459014Z","shell.execute_reply":"2024-12-17T05:12:32.466916Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fix_image_path(mass_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:12:33.811976Z","iopub.execute_input":"2024-12-17T05:12:33.812302Z","iopub.status.idle":"2024-12-17T05:12:34.017808Z","shell.execute_reply.started":"2024-12-17T05:12:33.812275Z","shell.execute_reply":"2024-12-17T05:12:34.016968Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mass_test = mass_test.rename(columns={'left or right breast': 'left_or_right_breast',\n                                      'image view': 'image_view',\n                                      'abnormality id': 'abnormality_id',\n                                      'abnormality type': 'abnormality_type',\n                                      'mass shape': 'mass_shape',\n                                      'mass margins': 'mass_margins',\n                                      'image file path': 'image_file_path',\n                                      'cropped image file path': 'cropped_image_file_path',\n                                      'ROI mask file path': 'ROI_mask_file_path'})\n# view renamed columns\nmass_test.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:12:35.118608Z","iopub.execute_input":"2024-12-17T05:12:35.119001Z","iopub.status.idle":"2024-12-17T05:12:35.133517Z","shell.execute_reply.started":"2024-12-17T05:12:35.118974Z","shell.execute_reply":"2024-12-17T05:12:35.132669Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"calc_train = calc_train.rename(columns={'left or right breast': 'left_or_right_breast',\n                                        'image view': 'image_view',\n                                        'abnormality id': 'abnormality_id',\n                                        'abnormality type': 'abnormality_type',\n                                        'mass shape': 'mass_shape',\n                                        'mass margins': 'mass_margins',\n                                        'image file path': 'image_file_path',\n                                        'cropped image file path': 'cropped_image_file_path',\n                                        'ROI mask file path': 'ROI_mask_file_path'})\n# view renamed columns\ncalc_train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:12:36.269621Z","iopub.execute_input":"2024-12-17T05:12:36.269999Z","iopub.status.idle":"2024-12-17T05:12:36.285994Z","shell.execute_reply.started":"2024-12-17T05:12:36.269971Z","shell.execute_reply":"2024-12-17T05:12:36.28512Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fix_image_path(calc_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:12:37.579525Z","iopub.execute_input":"2024-12-17T05:12:37.579895Z","iopub.status.idle":"2024-12-17T05:12:38.224733Z","shell.execute_reply.started":"2024-12-17T05:12:37.579865Z","shell.execute_reply":"2024-12-17T05:12:38.224008Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"calc_test = calc_test.rename(columns={'left or right breast': 'left_or_right_breast',\n                                      'image view': 'image_view',\n                                      'abnormality id': 'abnormality_id',\n                                      'abnormality type': 'abnormality_type',\n                                      'mass shape': 'mass_shape',\n                                      'mass margins': 'mass_margins',\n                                      'image file path': 'image_file_path',\n                                      'cropped image file path': 'cropped_image_file_path',\n                                      'ROI mask file path': 'ROI_mask_file_path'})\n# view renamed columns\ncalc_test.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:12:40.014737Z","iopub.execute_input":"2024-12-17T05:12:40.015283Z","iopub.status.idle":"2024-12-17T05:12:40.028736Z","shell.execute_reply.started":"2024-12-17T05:12:40.015251Z","shell.execute_reply":"2024-12-17T05:12:40.0278Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fix_image_path(calc_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:12:41.453294Z","iopub.execute_input":"2024-12-17T05:12:41.453624Z","iopub.status.idle":"2024-12-17T05:12:41.598184Z","shell.execute_reply.started":"2024-12-17T05:12:41.453596Z","shell.execute_reply":"2024-12-17T05:12:41.5974Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def display_images(dataset, column, number):\n    \"\"\"Displays images in dataset, handling missing files and converting formats.\"\"\"\n    \n    # create figure and axes\n    fig, axes = plt.subplots(1, number, figsize=(15, 5))\n    \n    # Loop through rows and display images\n    for index, (i, row) in enumerate(dataset.head(number).iterrows()):\n        image_path = row[column]\n        \n       # Check if image_path is valid (not None) and exists\n        if image_path is None or not os.path.exists(image_path):\n            # print(f\"File not found or invalid path: {image_path}\")\n            continue\n        \n        image = cv2.imread(image_path)\n        \n        # Handle case when image can't be read\n        if image is None:\n            # print(f\"Error reading image: {image_path}\")\n            continue\n        \n        # Convert BGR to RGB if needed (for correct color display)\n        if len(image.shape) == 3 and image.shape[2] == 3:\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        ax = axes[index]\n        ax.imshow(image, cmap='gray' if len(image.shape) == 2 else None)\n        ax.set_title(f\"{row['pathology']}\")\n        ax.axis('off')\n        print(np.array(image).shape)\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:12:43.59555Z","iopub.execute_input":"2024-12-17T05:12:43.595905Z","iopub.status.idle":"2024-12-17T05:12:43.602689Z","shell.execute_reply.started":"2024-12-17T05:12:43.595878Z","shell.execute_reply":"2024-12-17T05:12:43.601706Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('Full Mammograms:\\n')\ndisplay_images(mass_train, 'image_file_path', 5)\nprint('Cropped Mammograms:\\n')\ndisplay_images(mass_train, 'cropped_image_file_path', 5)\nprint('ROI_mask:\\n')\ndisplay_images(mass_train, 'ROI_mask_file_path', 5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:12:44.61754Z","iopub.execute_input":"2024-12-17T05:12:44.618185Z","iopub.status.idle":"2024-12-17T05:13:03.336667Z","shell.execute_reply.started":"2024-12-17T05:12:44.618154Z","shell.execute_reply":"2024-12-17T05:13:03.335776Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('Full Mammograms:\\n')\ndisplay_images(mass_test, 'image_file_path', 5)\nprint('Cropped Mammograms:\\n')\ndisplay_images(mass_test, 'cropped_image_file_path', 5)\nprint('ROI_mask:\\n')\ndisplay_images(mass_test, 'ROI_mask_file_path', 5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:13:38.221761Z","iopub.execute_input":"2024-12-17T05:13:38.222352Z","iopub.status.idle":"2024-12-17T05:13:54.487205Z","shell.execute_reply.started":"2024-12-17T05:13:38.222309Z","shell.execute_reply":"2024-12-17T05:13:54.486358Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('Full Mammograms:\\n')\ndisplay_images(calc_train, 'image_file_path', 5)\nprint('Cropped Mammograms:\\n')\ndisplay_images(calc_train, 'cropped_image_file_path', 5)\nprint('ROI_mask:\\n')\ndisplay_images(calc_train, 'ROI_mask_file_path', 5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:13:54.488482Z","iopub.execute_input":"2024-12-17T05:13:54.489196Z","iopub.status.idle":"2024-12-17T05:14:12.115464Z","shell.execute_reply.started":"2024-12-17T05:13:54.48915Z","shell.execute_reply":"2024-12-17T05:14:12.114703Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('Full Mammograms:\\n')\ndisplay_images(calc_test, 'image_file_path', 5)\nprint('Cropped Mammograms:\\n')\ndisplay_images(calc_test, 'cropped_image_file_path', 5)\nprint('ROI_mask:\\n')\ndisplay_images(calc_test, 'ROI_mask_file_path', 5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:14:12.116738Z","iopub.execute_input":"2024-12-17T05:14:12.117362Z","iopub.status.idle":"2024-12-17T05:14:14.126609Z","shell.execute_reply.started":"2024-12-17T05:14:12.117321Z","shell.execute_reply":"2024-12-17T05:14:14.125932Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Combining the datasets into one.","metadata":{}},{"cell_type":"code","source":"# full_dataset = pd.concat([mass_train, mass_test, calc_train, calc_test], axis=0)\n# try next time\n\nfull_dataset = pd.concat([calc_train, calc_test], axis=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:14:53.02618Z","iopub.execute_input":"2024-12-17T05:14:53.026972Z","iopub.status.idle":"2024-12-17T05:14:53.032547Z","shell.execute_reply.started":"2024-12-17T05:14:53.026938Z","shell.execute_reply":"2024-12-17T05:14:53.031577Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(full_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:15:03.892594Z","iopub.execute_input":"2024-12-17T05:15:03.893386Z","iopub.status.idle":"2024-12-17T05:15:03.899982Z","shell.execute_reply.started":"2024-12-17T05:15:03.893336Z","shell.execute_reply":"2024-12-17T05:15:03.898972Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_mapper = {'MALIGNANT': 1, 'BENIGN': 0, 'BENIGN_WITHOUT_CALLBACK': 0} ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:15:13.198801Z","iopub.execute_input":"2024-12-17T05:15:13.199144Z","iopub.status.idle":"2024-12-17T05:15:13.203034Z","shell.execute_reply.started":"2024-12-17T05:15:13.199116Z","shell.execute_reply":"2024-12-17T05:15:13.202178Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"target_size = (224, 224, 3)\n\n# Apply class mapper to pathology column\nfull_dataset['labels'] = full_dataset['pathology'].replace(class_mapper).infer_objects(copy=False)\n\nfull_images = np.array(full_dataset[full_dataset[\"image_file_path\"].notna()][\"image_file_path\"].tolist())\nfull_labels = np.array(full_dataset[full_dataset[\"image_file_path\"].notna()][\"labels\"].tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:15:17.089729Z","iopub.execute_input":"2024-12-17T05:15:17.090063Z","iopub.status.idle":"2024-12-17T05:15:17.103307Z","shell.execute_reply.started":"2024-12-17T05:15:17.090036Z","shell.execute_reply":"2024-12-17T05:15:17.102409Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(full_images)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:15:18.830928Z","iopub.execute_input":"2024-12-17T05:15:18.831732Z","iopub.status.idle":"2024-12-17T05:15:18.836705Z","shell.execute_reply.started":"2024-12-17T05:15:18.831691Z","shell.execute_reply":"2024-12-17T05:15:18.835904Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# If full_labels is a NumPy array, convert it to a Pandas series\nfull_labels_series = pd.Series(full_labels)\n\n# Count the occurrences of each class\nlabel_counts = full_labels_series.value_counts()\n\n# Assuming 0 = benign and 1 = malignant\nbenign_count = label_counts.get(0, 0)\nmalignant_count = label_counts.get(1, 0)\n\nprint(f\"Benign images: {benign_count}\")\nprint(f\"Malignant images: {malignant_count}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:15:20.776251Z","iopub.execute_input":"2024-12-17T05:15:20.776576Z","iopub.status.idle":"2024-12-17T05:15:20.785989Z","shell.execute_reply.started":"2024-12-17T05:15:20.776548Z","shell.execute_reply":"2024-12-17T05:15:20.785038Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_classes = len(full_dataset['labels'].unique())\nnum_classes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:15:21.800239Z","iopub.execute_input":"2024-12-17T05:15:21.800924Z","iopub.status.idle":"2024-12-17T05:15:21.807526Z","shell.execute_reply.started":"2024-12-17T05:15:21.80089Z","shell.execute_reply":"2024-12-17T05:15:21.806573Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_names = ['Benign', 'Malignant']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:15:22.859334Z","iopub.execute_input":"2024-12-17T05:15:22.859982Z","iopub.status.idle":"2024-12-17T05:15:22.8636Z","shell.execute_reply.started":"2024-12-17T05:15:22.859948Z","shell.execute_reply":"2024-12-17T05:15:22.862828Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check the distribution of labels\nlabel_counts = full_dataset['labels'].value_counts()\nprint(label_counts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:15:24.052939Z","iopub.execute_input":"2024-12-17T05:15:24.053241Z","iopub.status.idle":"2024-12-17T05:15:24.05868Z","shell.execute_reply.started":"2024-12-17T05:15:24.053217Z","shell.execute_reply":"2024-12-17T05:15:24.057837Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define a function for data augmentation\ndef augment_image(image):\n    # Apply data augmentation using tf.image functions\n    image = tf.image.random_flip_left_right(image)\n#     image = tf.image.random_flip_up_down(image)\n    image = tf.image.random_brightness(image, max_delta=0.3)\n    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n    image = tf.image.random_saturation(image, lower=0.8, upper=1.2)\n    return image\n\n# Function to resize image to (224, 224, 3)\ndef resize_image(image_tensor):\n    return tf.image.resize(image_tensor, [224, 224])\n\n# Function to balance classes by augmenting images\ndef copy_images_with_unique_filenames(images, labels, source, destination, target_count=None):\n    \"\"\"\n    Copy images from source to destination in subfolders '0' and '1',\n    ensuring unique filenames and applying data augmentation and balancing.\n    \"\"\"\n    benign_images = 0\n    malignant_images = 0\n    skipped_images = []\n\n    # Create the destination subfolders '0' and '1'\n    category_dest_dir_zero = os.path.join(destination, '0')\n    os.makedirs(category_dest_dir_zero, exist_ok=True)\n\n    category_dest_dir_one = os.path.join(destination, '1')\n    os.makedirs(category_dest_dir_one, exist_ok=True)\n\n    benign_images_list = []\n    malignant_images_list = []\n\n    for i, (image, label) in enumerate(zip(images, labels)):\n#         img_name = data_frame.REFNUM[i]\n#         abs_path = os.path.join(source, img_name + '.pgm')\n\n        if os.path.exists(image):\n            try:\n                # Generate a unique filename\n                filename = os.path.basename(image)\n                unique_filename = f\"{uuid.uuid4().hex}_{filename}\"\n        \n                # Open the image using PIL\n                with Image.open(image) as img:\n                    # Convert the image to RGB mode (for saving as JPEG)\n                    img = img.convert('RGB')\n                    # Augment the image (convert it to a Tensor first)\n                    img_tensor = tf.convert_to_tensor(img)\n                    # Resize the image to (224, 224, 3)\n                    resized_img_tensor = resize_image(img_tensor)\n                    augmented_image_tensor = augment_image(resized_img_tensor)\n                    # Convert Tensor back to PIL image for saving\n                    augmented_image = tf.keras.preprocessing.image.array_to_img(augmented_image_tensor)\n\n                    if label == 0:\n                        benign_images_list.append(unique_filename)\n                        dest_path = os.path.join(category_dest_dir_zero, unique_filename)\n#                         augmented_image.save(dest_path, 'JPEG')\n                        augmented_image.save(dest_path, 'JPEG')\n                        benign_images += 1\n\n                    elif label == 1:\n                        malignant_images_list.append(unique_filename)\n                        dest_path = os.path.join(category_dest_dir_one, unique_filename)\n#                         augmented_image.save(dest_path, 'JPEG')\n                        augmented_image.save(dest_path, 'JPEG')\n                        malignant_images += 1\n                        \n#                 del img, img_tensor, resized_img_tensor, augmented_image_tensor, augmented_image\n#                 gc.collect()\n            except Exception as e:\n                print(f\"Error copying image {image}: {e}\")\n                skipped_images.append(image)\n        else:\n            print(f\"Image not found: {image}\")\n            skipped_images.append(image)\n\n    # If balancing is needed, duplicate/augment images from the smaller class\n    benign_count = len(benign_images_list)\n    malignant_count = len(malignant_images_list)\n\n    if benign_count < malignant_count:\n#         augment_and_save_images(benign_images_list, category_dest_dir_zero, target_count - benign_count)\n        augment_and_save_images(benign_images_list, category_dest_dir_zero, malignant_count - benign_count)\n\n    elif malignant_count < benign_count:\n        augment_and_save_images(malignant_images_list, category_dest_dir_one, benign_count - malignant_count)\n\n    augment_and_save_images(benign_images_list, category_dest_dir_zero, target_count)\n    augment_and_save_images(malignant_images_list, category_dest_dir_one, target_count)\n\n    print(f\"\\nCopying complete.\")\n    print(f\"Benign images copied (label 0): {benign_images}\")\n    print(f\"Benign count (label 0): {benign_count}\")\n    print(f\"Malignant images copied (label 1): {malignant_images}\")\n    print(f\"Malignant count (label 1): {malignant_count}\")\n    print(f\"Total skipped images: {len(skipped_images)}\")\n    if skipped_images:\n        print(\"Skipped images:\")\n        for img in skipped_images:\n            print(img)\n            \n\n# Function to augment and save images to balance the dataset\ndef augment_and_save_images(images_list, destination_dir, num_augments):\n    \"\"\"\n    Augment and save images to balance the dataset.\n    \"\"\"\n    for i in range(num_augments):\n        img_name = random.choice(images_list)\n        abs_path = os.path.join(destination_dir, img_name)\n\n        try:\n            with Image.open(abs_path) as img:\n                img = img.convert('RGB')\n                # Augment the image\n                img_tensor = tf.convert_to_tensor(img)\n                # Resize the image\n#                 resized_img_tensor = resize_image(img_tensor)\n                augmented_image_tensor = augment_image(img_tensor)\n                # Convert Tensor back to PIL image for saving\n                augmented_image = tf.keras.preprocessing.image.array_to_img(augmented_image_tensor)\n                # Remove the original extension from img_name 1-285.jpg --> 1-285\n                img_name_without_ext = os.path.splitext(img_name)[0]\n                # Save augmented image with a unique name\n                augmented_image.save(os.path.join(destination_dir, img_name_without_ext + f'_aug{i}.jpg'), 'JPEG')\n            \n        except Exception as e:\n            print(f\"Error augmenting image {abs_path}: {e}\")\n\nsource_dir = \"/kaggle/input/cbis-ddsm-breast-cancer-image-dataset/jpeg\"\ndestination_dir = \"/kaggle/working/merged_images\"\n\n# target_count=0 meaning no Augmentation, There's just Data-Balance\ntarget_count = (len(full_labels) * 3) - len(full_labels)\ncopy_images_with_unique_filenames(full_images, full_labels, source_dir, destination_dir, target_count)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:15:25.898695Z","iopub.execute_input":"2024-12-17T05:15:25.899008Z","iopub.status.idle":"2024-12-17T05:23:58.689031Z","shell.execute_reply.started":"2024-12-17T05:15:25.898983Z","shell.execute_reply":"2024-12-17T05:23:58.688112Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check the number of images in each class folder after merging\nzero_class_count = len(os.listdir(\"/kaggle/working/merged_images/0\"))\none_class_count  = len(os.listdir(\"/kaggle/working/merged_images/1\"))\n\nprint(f\"Number of images in class 0: {zero_class_count}\")\nprint(f\"Number of images in class 1: {one_class_count}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:24:19.92009Z","iopub.execute_input":"2024-12-17T05:24:19.920754Z","iopub.status.idle":"2024-12-17T05:24:19.9318Z","shell.execute_reply.started":"2024-12-17T05:24:19.920714Z","shell.execute_reply":"2024-12-17T05:24:19.930927Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\n\ndata_dir = '/kaggle/working/merged_images'  # Update with the dataset path\n\n# Create a dataset for the entire data to use for split\nfull_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n    data_dir,\n    labels='inferred',\n    label_mode='categorical',\n    # image_size=(224, 224),\n    image_size=(224, 224),\n    seed=50,\n    shuffle=True,\n    batch_size=13\n)\n# Calculate the total number of samples\ntotal_samples = tf.data.experimental.cardinality(full_dataset).numpy()\n\ntrain_size = int(0.8 * total_samples)                 # 70% for training\nval_size   = int(0.15 * total_samples)                # 20% for validation\ntest_size = total_samples - train_size - val_size     # 10% for testing\n\n# Create train, validation, and test datasets\ntrain_dataset       = full_dataset.take(train_size)\nvalidation_dataset  = full_dataset.skip(train_size).take(val_size)\ntest_dataset        = full_dataset.skip(train_size + val_size)\n\ntrain_dataset      = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\nvalidation_dataset = validation_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\ntest_dataset       = test_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n\n# Print the number of samples in each dataset\nprint(f\"Train samples:      {train_size}     batches(13) ==> {train_size*13}\")\nprint(f\"Validation samples: {val_size}       batches(13) ==> {val_size*13}\")\nprint(f\"Test samples:       {test_size}      batches(13) ==> {test_size*13}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:24:23.92913Z","iopub.execute_input":"2024-12-17T05:24:23.92982Z","iopub.status.idle":"2024-12-17T05:24:25.700002Z","shell.execute_reply.started":"2024-12-17T05:24:23.929767Z","shell.execute_reply":"2024-12-17T05:24:25.699136Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.applications import ResNet50\n\ndef try_model():\n    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\n    # Freeze all layers initially\n    for layer in base_model.layers:\n        layer.trainable = False\n\n    # Calculate the index to start unfreezing layers\n    from_index = int(np.round((len(base_model.layers) - 1) * (1.0 - 50.0 / 100.0)))\n\n    # Unfreeze layers from the calculated index onwards\n    for layer in base_model.layers[from_index:]:\n        layer.trainable = True\n\n    # Add custom layers on top (Upper-Layers)\n    x = base_model.output\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(1024, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    predictions = Dense(2, activation='softmax')(x)  # Assuming binary classification\n\n    model = Model(inputs=base_model.input, outputs=predictions)\n    \n    # # Clear the base model from memory if needed (optional)\n    # del model_dict, base_model, from_index, x, predictions;    gc.collect()\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T05:38:36.160099Z","iopub.execute_input":"2024-12-08T05:38:36.160477Z","iopub.status.idle":"2024-12-08T05:38:36.169161Z","shell.execute_reply.started":"2024-12-08T05:38:36.160444Z","shell.execute_reply":"2024-12-08T05:38:36.168394Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.metrics import Precision, Recall\nfrom tensorflow.keras.optimizers import Adam\n\ntrymodel = try_model()\n\ntrymodel.compile(optimizer=Adam(learning_rate=1e-4),\n                      loss='categorical_crossentropy',\n                      metrics=['accuracy', Precision(name='precision'), Recall(name='recall')])  # Compile the model\ntrymodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T05:38:40.9953Z","iopub.execute_input":"2024-12-08T05:38:40.9957Z","iopub.status.idle":"2024-12-08T05:38:45.425477Z","shell.execute_reply.started":"2024-12-08T05:38:40.99567Z","shell.execute_reply":"2024-12-08T05:38:45.424755Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = trymodel.fit(\n            train_dataset,\n            validation_data=validation_dataset,\n            batch_size=13,\n            epochs=7\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T05:39:06.525368Z","iopub.execute_input":"2024-12-08T05:39:06.525728Z","iopub.status.idle":"2024-12-08T05:43:08.33364Z","shell.execute_reply.started":"2024-12-08T05:39:06.525699Z","shell.execute_reply":"2024-12-08T05:43:08.332799Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Moving onto our second model!","metadata":{}},{"cell_type":"code","source":"# Define the second model with adjusted hyperparameters\ndef try_model_v2():\n    base_model_v2 = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\n    # Freeze all layers initially\n    for layer in base_model_v2.layers:\n        layer.trainable = False\n\n    # Unfreeze 60% of layers instead of 50%\n    from_index_v2 = int(np.round((len(base_model_v2.layers) - 1) * (1.0 - 60.0 / 100.0)))\n\n    # Unfreeze layers from the calculated index onwards\n    for layer in base_model_v2.layers[from_index_v2:]:\n        layer.trainable = True\n\n    # Add custom layers on top (Upper-Layers)\n    x_v2 = base_model_v2.output\n    x_v2 = GlobalAveragePooling2D()(x_v2)\n    x_v2 = Dense(512, activation='relu')(x_v2)  # Reduced Dense layer size from 1024 to 512\n    x_v2 = Dropout(0.3)(x_v2)  # Lowered dropout rate from 0.5 to 0.3\n    predictions_v2 = Dense(2, activation='softmax')(x_v2)  # Assuming binary classification\n\n    model_v2 = Model(inputs=base_model_v2.input, outputs=predictions_v2)\n    \n    return model_v2\n\n# Create and compile the second model\ntrymodel_v2 = try_model_v2()\ntrymodel_v2.compile(optimizer=Adam(learning_rate=5e-5),  # Adjusted learning rate to 5e-5\n                    loss='categorical_crossentropy',\n                    metrics=['accuracy', Precision(name='precision_v2'), Recall(name='recall_v2')])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T05:43:28.14489Z","iopub.execute_input":"2024-12-08T05:43:28.14521Z","iopub.status.idle":"2024-12-08T05:43:29.166018Z","shell.execute_reply.started":"2024-12-08T05:43:28.145183Z","shell.execute_reply":"2024-12-08T05:43:29.165322Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train the second model\nhistory_v2 = trymodel_v2.fit(\n            train_dataset,\n            validation_data=validation_dataset,\n            batch_size=13,\n            epochs=7\n        )\n\n# Summarize the second model\ntrymodel_v2.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T05:43:37.574576Z","iopub.execute_input":"2024-12-08T05:43:37.575241Z","iopub.status.idle":"2024-12-08T05:47:57.560774Z","shell.execute_reply.started":"2024-12-08T05:43:37.575206Z","shell.execute_reply":"2024-12-08T05:47:57.559872Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plot the validation loss\nplt.plot(history_v2.history['val_loss'], label='Validation Loss')\nplt.plot(history_v2.history['loss'], label='Training Loss')\nplt.title('Validation and Training Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n# Plot the validation accuracy\nplt.plot(history_v2.history['val_accuracy'], label='Validation Accuracy')\nplt.plot(history_v2.history['accuracy'], label='Training Accuracy')\nplt.title('Validation and Training Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T03:38:37.771252Z","iopub.execute_input":"2024-12-14T03:38:37.772018Z","iopub.status.idle":"2024-12-14T03:38:38.218831Z","shell.execute_reply.started":"2024-12-14T03:38:37.771985Z","shell.execute_reply":"2024-12-14T03:38:38.217647Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Lastly, our third model!","metadata":{}},{"cell_type":"code","source":"# Define the third model with different hyperparameters\ndef try_model_v3():\n    base_model_v3 = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\n    # Freeze all layers initially\n    for layer in base_model_v3.layers:\n        layer.trainable = False\n\n    # Unfreeze 70% of layers (more layers unfreezed compared to v1 and v2)\n    from_index_v3 = int(np.round((len(base_model_v3.layers) - 1) * (1.0 - 70.0 / 100.0)))\n\n    # Unfreeze layers from the calculated index onwards\n    for layer in base_model_v3.layers[from_index_v3:]:\n        layer.trainable = True\n\n    # Add custom layers on top (Upper-Layers)\n    x_v3 = base_model_v3.output\n    x_v3 = GlobalAveragePooling2D()(x_v3)\n    x_v3 = Dense(1024, activation='relu')(x_v3)  # Return Dense layer size to 1024\n    x_v3 = Dropout(0.4)(x_v3)  # Moderate dropout rate (between v1 and v2)\n    predictions_v3 = Dense(2, activation='softmax')(x_v3)  # Assuming binary classification\n\n    model_v3 = Model(inputs=base_model_v3.input, outputs=predictions_v3)\n    \n    return model_v3\n\n# Create and compile the third model\ntrymodel_v3 = try_model_v3()\ntrymodel_v3.compile(optimizer=RMSprop(learning_rate=1e-4),  # Change optimizer to RMSprop and learning rate back to 1e-4\n                    loss='categorical_crossentropy',\n                    metrics=['accuracy', Precision(name='precision_v3'), Recall(name='recall_v3')])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T05:48:34.693486Z","iopub.execute_input":"2024-12-08T05:48:34.694278Z","iopub.status.idle":"2024-12-08T05:48:35.696422Z","shell.execute_reply.started":"2024-12-08T05:48:34.694246Z","shell.execute_reply":"2024-12-08T05:48:35.695706Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train the third model\nhistory_v3 = trymodel_v3.fit(\n            train_dataset,\n            validation_data=validation_dataset,\n            batch_size=13,\n            epochs=7\n        )\n\n# Summarize the third model\ntrymodel_v3.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T05:48:45.702144Z","iopub.execute_input":"2024-12-08T05:48:45.70277Z","iopub.status.idle":"2024-12-08T05:53:03.872958Z","shell.execute_reply.started":"2024-12-08T05:48:45.702732Z","shell.execute_reply":"2024-12-08T05:53:03.87213Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plot accuracy comparison\nplt.figure(figsize=(12, 8))\n\n# Accuracy\nplt.subplot(2, 2, 1)\nplt.plot(history.history['accuracy'], label='Model 1 Accuracy')\nplt.plot(history_v2.history['accuracy'], label='Model 2 Accuracy')\nplt.plot(history_v3.history['accuracy'], label='Model 3 Accuracy')\nplt.title('Training Accuracy')\nplt.legend()\n\n# Validation Accuracy\nplt.subplot(2, 2, 2)\nplt.plot(history.history['val_accuracy'], label='Model 1 Val Accuracy')\nplt.plot(history_v2.history['val_accuracy'], label='Model 2 Val Accuracy')\nplt.plot(history_v3.history['val_accuracy'], label='Model 3 Val Accuracy')\nplt.title('Validation Accuracy')\nplt.legend()\n\n# Loss\nplt.subplot(2, 2, 3)\nplt.plot(history.history['loss'], label='Model 1 Loss')\nplt.plot(history_v2.history['loss'], label='Model 2 Loss')\nplt.plot(history_v3.history['loss'], label='Model 3 Loss')\nplt.title('Training Loss')\nplt.legend()\n\n# Validation Loss\nplt.subplot(2, 2, 4)\nplt.plot(history.history['val_loss'], label='Model 1 Val Loss')\nplt.plot(history_v2.history['val_loss'], label='Model 2 Val Loss')\nplt.plot(history_v3.history['val_loss'], label='Model 3 Val Loss')\nplt.title('Validation Loss')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T05:54:43.138119Z","iopub.execute_input":"2024-12-08T05:54:43.138508Z","iopub.status.idle":"2024-12-08T05:54:43.963762Z","shell.execute_reply.started":"2024-12-08T05:54:43.138474Z","shell.execute_reply":"2024-12-08T05:54:43.962819Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot Precision\nplt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1)\nplt.plot(history.history['precision'], label='Model 1 Precision')\nplt.plot(history_v2.history['precision_v2'], label='Model 2 Precision')\nplt.plot(history_v3.history['precision_v3'], label='Model 3 Precision')\nplt.title('Training Precision')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['val_precision'], label='Model 1 Val Precision')\nplt.plot(history_v2.history['val_precision_v2'], label='Model 2 Val Precision')\nplt.plot(history_v3.history['val_precision_v3'], label='Model 3 Val Precision')\nplt.title('Validation Precision')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# Plot Recall\nplt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1)\nplt.plot(history.history['recall'], label='Model 1 Recall')\nplt.plot(history_v2.history['recall_v2'], label='Model 2 Recall')\nplt.plot(history_v3.history['recall_v3'], label='Model 3 Recall')\nplt.title('Training Recall')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['val_recall'], label='Model 1 Val Recall')\nplt.plot(history_v2.history['val_recall_v2'], label='Model 2 Val Recall')\nplt.plot(history_v3.history['val_recall_v3'], label='Model 3 Val Recall')\nplt.title('Validation Recall')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T05:54:55.77987Z","iopub.execute_input":"2024-12-08T05:54:55.780226Z","iopub.status.idle":"2024-12-08T05:54:56.790637Z","shell.execute_reply.started":"2024-12-08T05:54:55.780192Z","shell.execute_reply":"2024-12-08T05:54:56.789796Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compare the validation precision and recall to determine the best model\nbest_model = \"Model 1\"\nbest_val_precision = max(history.history['val_precision'], history_v2.history['val_precision_v2'], history_v3.history['val_precision_v3'])\nbest_val_recall = max(history.history['val_recall'], history_v2.history['val_recall_v2'], history_v3.history['val_recall_v3'])\n\n# Display the best model based on precision and recall\nif best_val_precision > best_val_recall:\n    best_model = \"Model 1\"\nelif best_val_precision < best_val_recall:\n    best_model = \"Model 2\"\nelse:\n    best_model = \"Model 3\"\n\nprint(f\"The best model is: {best_model}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T05:55:09.141755Z","iopub.execute_input":"2024-12-08T05:55:09.142105Z","iopub.status.idle":"2024-12-08T05:55:09.148261Z","shell.execute_reply.started":"2024-12-08T05:55:09.142072Z","shell.execute_reply":"2024-12-08T05:55:09.147368Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.applications import MobileNet\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.metrics import Precision, Recall\nfrom tensorflow.keras.optimizers import Adam\nimport numpy as np\n\ndef try_model():\n    # Load the base model with MobileNet\n    base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\n    # Freeze all layers initially\n    for layer in base_model.layers:\n        layer.trainable = False\n\n    # Calculate the index to start unfreezing layers\n    from_index = int(np.round((len(base_model.layers) - 1) * (1.0 - 50.0 / 100.0)))\n\n    # Unfreeze layers from the calculated index onwards\n    for layer in base_model.layers[from_index:]:\n        layer.trainable = True\n\n    # Add custom layers on top (Upper-Layers)\n    x = base_model.output\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(1024, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    predictions = Dense(2, activation='softmax')(x)  # Assuming binary classification\n\n    model = Model(inputs=base_model.input, outputs=predictions)\n    return model\n\n# Create the model\ntrymodel = try_model()\n\n# Compile the model\ntrymodel.compile(optimizer=Adam(learning_rate=1e-4),\n                 loss='categorical_crossentropy',\n                 metrics=['accuracy', Precision(name='precision'), Recall(name='recall')])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T03:39:04.407455Z","iopub.execute_input":"2024-12-14T03:39:04.407816Z","iopub.status.idle":"2024-12-14T03:39:05.332744Z","shell.execute_reply.started":"2024-12-14T03:39:04.407786Z","shell.execute_reply":"2024-12-14T03:39:05.331878Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# Train the model\nhistory_V = trymodel.fit(\n    train_dataset,\n    validation_data=validation_dataset,\n    batch_size=13,\n    epochs=7\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T03:44:22.051549Z","iopub.execute_input":"2024-12-14T03:44:22.051939Z","iopub.status.idle":"2024-12-14T03:45:42.219899Z","shell.execute_reply.started":"2024-12-14T03:44:22.051909Z","shell.execute_reply":"2024-12-14T03:45:42.219158Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plot the validation loss\nplt.plot(history_V.history['val_loss'], label='Validation Loss')\nplt.plot(history_V.history['loss'], label='Training Loss')\nplt.title('Validation and Training Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n# Plot the validation accuracy\nplt.plot(history_V.history['val_accuracy'], label='Validation Accuracy')\nplt.plot(history_V.history['accuracy'], label='Training Accuracy')\nplt.title('Validation and Training Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T03:45:47.790181Z","iopub.execute_input":"2024-12-14T03:45:47.790617Z","iopub.status.idle":"2024-12-14T03:45:48.163284Z","shell.execute_reply.started":"2024-12-14T03:45:47.790548Z","shell.execute_reply":"2024-12-14T03:45:48.162435Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.metrics import Precision, Recall\nfrom tensorflow.keras.optimizers import Adam\nimport numpy as np\n\ndef try_model_vgg16():\n    # Load the base model with VGG16\n    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\n    # Freeze all layers initially\n    for layer in base_model.layers:\n        layer.trainable = False\n\n    # Calculate the index to start unfreezing layers\n    from_index = int(np.round((len(base_model.layers) - 1) * (1.0 - 50.0 / 100.0)))\n\n    # Unfreeze layers from the calculated index onwards\n    for layer in base_model.layers[from_index:]:\n        layer.trainable = True\n\n    # Add custom layers on top (Upper-Layers)\n    x = base_model.output\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(1024, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    predictions = Dense(2, activation='softmax')(x)  # Assuming binary classification\n\n    model = Model(inputs=base_model.input, outputs=predictions)\n    return model\n\n# Create the model\ntrymodel_vgg16 = try_model_vgg16()\n\n# Compile the model\ntrymodel_vgg16.compile(optimizer=Adam(learning_rate=1e-4),\n                       loss='categorical_crossentropy',\n                       metrics=['accuracy', Precision(name='precision'), Recall(name='recall')])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:26:02.115561Z","iopub.execute_input":"2024-12-17T05:26:02.115947Z","iopub.status.idle":"2024-12-17T05:26:06.566256Z","shell.execute_reply.started":"2024-12-17T05:26:02.115919Z","shell.execute_reply":"2024-12-17T05:26:06.565574Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train the model\nhistory_V2 = trymodel_vgg16.fit(\n    train_dataset,\n    validation_data=validation_dataset,\n    batch_size=13,\n    epochs=7\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:26:06.567534Z","iopub.execute_input":"2024-12-17T05:26:06.567795Z","iopub.status.idle":"2024-12-17T05:30:17.739849Z","shell.execute_reply.started":"2024-12-17T05:26:06.567772Z","shell.execute_reply":"2024-12-17T05:30:17.738917Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plot the validation loss\nplt.plot(history_V2.history['val_loss'], label='Validation Loss')\nplt.plot(history_V2.history['loss'], label='Training Loss')\nplt.title('Validation and Training Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n# Plot the validation accuracy\nplt.plot(history_V2.history['val_accuracy'], label='Validation Accuracy')\nplt.plot(history_V2.history['accuracy'], label='Training Accuracy')\nplt.title('Validation and Training Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:30:41.723789Z","iopub.execute_input":"2024-12-17T05:30:41.72412Z","iopub.status.idle":"2024-12-17T05:30:42.213511Z","shell.execute_reply.started":"2024-12-17T05:30:41.724093Z","shell.execute_reply":"2024-12-17T05:30:42.212664Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\n# Save the third model (v3) to a file\ntrymodel_vgg16.save('model_vgg16.h5')\n\n# Optionally, load the model back to verify\ntrymodel_vgg16 = load_model('model_vgg16.h5')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T05:30:49.035047Z","iopub.execute_input":"2024-12-17T05:30:49.036073Z","iopub.status.idle":"2024-12-17T05:30:49.418166Z","shell.execute_reply.started":"2024-12-17T05:30:49.036012Z","shell.execute_reply":"2024-12-17T05:30:49.417293Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}